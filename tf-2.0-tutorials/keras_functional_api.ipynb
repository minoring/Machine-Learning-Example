{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keras_functional_api.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZurMVx9me6d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "# Importing incompatible module.\n",
        "try:\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.keras.backend.clear_session() # For easy reset of notebook state."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGEM_aFsoFfG",
        "colab_type": "text"
      },
      "source": [
        "## Introduction\n",
        "The Functional API is a way to create models that is more flexible than `Sequential`: it can handle models with non-linear topology, models with shared layers, and models with multiple inputs or outputs.\n",
        "\n",
        "It's based on the idea that a deep learning model is usually a directed acyclic graph(DAG) of layers. The Functional API a set of tools for **building graphs of layers**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OlScPPrm4G9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To build model with the functional API, we would start by creating an input node:\n",
        "from tensorflow import keras\n",
        "\n",
        "inputs = keras.Input(shape=(784, ), name='input10')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuiUZT96o_nK",
        "colab_type": "text"
      },
      "source": [
        "Here we just the shape of our data: 784-dimensional vectors. Note that the **batch size is always omited**, we only specify the shape of each sample. For an input meant for images of shape `(32, 32, 3)`, we would used:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vnkLELLo5q6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_inputs = keras.Input(shape=(32, 32, 3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-ZAcZRZpQZi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# What gets returned, `inputs` contains information about the shape\n",
        "# and dtype of the input data that you expect to feed to your model:\n",
        "inputs.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaTFgr6Mpdlj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs.dtype"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ur6saU4fpnVU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8jtVW_DpgIW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Layer(Node)는 기대하는 tensor의 shape와 dtype을 갖고있다."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pslkk5eNpv9s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a new node in the graph of layers by calling a layer\n",
        "# on this inputs object:\n",
        "# 그래프의 레이어의 새 노드를 만들고 싶으면, 호출하면 된다. \n",
        "from tensorflow.keras import layers\n",
        "\n",
        "dense = layers.Dense(64, activation='relu')\n",
        "x = dense(inputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6y2sCJkqZKm",
        "colab_type": "text"
      },
      "source": [
        "The \"layer call\" action is like drawing an arrow from \"inputs\" to this layer. We're \"passing\" the inputs to the `dense` layer, and out we get `x`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWzSYH4TqXPG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's add a few more layers to our graph of layers:\n",
        "x = layers.Dense(64, activation='relu')(x)\n",
        "outputs = layers.Dense(10, activation='softmax')(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bn3NF_tDq4hX",
        "colab_type": "text"
      },
      "source": [
        "At this point, we can create a **`Model`** by specifying its inputs and outputs in the graph of layers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69Q_D3iwq327",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "# DAG에서 input output을 지정해주면, 그부분을 똑 떼어내서 모델을 만들어 주는건가."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQWKy9GDrFE3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To recap,\n",
        "inputs = keras.Input(shape=(784, ), name='image')\n",
        "dense_1 = keras.layers.Dense(64, activation='relu', name='dense_1')(inputs)\n",
        "dense_2 = keras.layers.Dense(64, activation='relu', name='dense_2')(dense_1)\n",
        "outputs = keras.layers.Dense(10, activation='softmax', name='outputs')(dense_2)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs, name='mnist_model')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9x0h8hGVr5ha",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHMLfz1XsLSO",
        "colab_type": "text"
      },
      "source": [
        "We can also plot the model as a graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7wTIlzwr7Ao",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "keras.utils.plot_model(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "id-LJAcZsQuM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Display the output shapes of each layer in the plotted graph:\n",
        "keras.utils.plot_model(model, show_shapes=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzBTmUqtszug",
        "colab_type": "text"
      },
      "source": [
        "This figure and the code we wrote are virtually identical. In the code version,the connection arrows are simple replaced by the call operation.\n",
        "\n",
        "A \"graph of layers\" is a very intuitive mental image for a deep learning model, and the functional API is a way to create models that closely mirrors this mental image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooiy_lZAuUmP",
        "colab_type": "text"
      },
      "source": [
        "## Training, evaluation, and inference\n",
        "Exactly in the same way for models built using the Functional API as for Sequential models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvNgClLqssoU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load MNIST image data, reshape it into vectors, fit the model on the data\n",
        "# (while monitoring performacne on a validation split), and finally we evaluate out model\n",
        "# on the test data\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "x_train = x_train.reshape(x_train.shape[0], -1).astype('float32') / 255\n",
        "x_test = x_test.reshape(x_test.shape[0], -1).astype('float32') / 255\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MfF6l6su_5g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train[0:10] # Not one-hot encoding."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRsXQAOVvJT5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer=keras.optimizers.RMSprop(),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pf6w32iJvZRO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(x_train, y_train, batch_size=64,\n",
        "                    epochs=5, validation_split=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsap4FzbvmON",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_scores = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss: ', test_scores[0])\n",
        "print('Test accuracy: ', test_scores[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-9Ei9PnwVl_",
        "colab_type": "text"
      },
      "source": [
        "## Saving and serialization\n",
        "Saving and serialization work exactly in the same way for models built using the Functional API as for Sequential models.\n",
        "\n",
        "To standard way to save a Functional model is to call `model.save()` to save the whole model into a single file. You can later recreate the same model from this file, even if you no longer have access to the code that create the model.\n",
        "\n",
        "This file includes:\n",
        "- The model's architecture\n",
        "- The model's weights values (which were learned during training)\n",
        "- The model's training config (what you passed to `compile`), if any\n",
        "- The optimizer and its state, if any (this enables you to restart training where you left off)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClTTM7dwwANp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('model.h5')\n",
        "del model\n",
        "\n",
        "model = keras.models.load_model('model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yISJLGtye1s",
        "colab_type": "text"
      },
      "source": [
        "## Using the same graph of layers to define multiple models\n",
        "In functional API, models are created by specifying their inputs and outpus in a graph of layers. That means that a single graph of layers can be used to generate multiple models.\n",
        "\n",
        "In the example below, we use the same stack of layers to instantiate two models: an `encoder` model that turns image input into 16-dimensional vectors, and end-to-end `autoencoder` model for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUD2Al5nxjZU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder_input = keras.Input(shape=(28, 28, 1), name='img')\n",
        "x = layers.Conv2D(filters=16, kernel_size=3, activation='relu')(encoder_input)\n",
        "x = layers.Conv2D(32, 3, activation='relu')(x)\n",
        "x = layers.MaxPooling2D(pool_size=3)(x)\n",
        "x = layers.Conv2D(32, 3, activation='relu')(x)\n",
        "x = layers.Conv2D(16, 3, activation='relu')(x)\n",
        "encoder_output = layers.GlobalMaxPooling2D()(x)\n",
        "\n",
        "encoder = keras.Model(encoder_input, encoder_output, name='encoder')\n",
        "encoder.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zk72Wjy00Ln",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = layers.Reshape((4, 4, 1))(encoder_output)\n",
        "x = layers.Conv2DTranspose(16, 3, activation='relu')(x)\n",
        "x = layers.Conv2DTranspose(32, 3, activation='relu')(x)\n",
        "x = layers.UpSampling2D(3)(x)\n",
        "x = layers.Conv2DTranspose(16, 3, activation='relu')(x)\n",
        "decoder_output = layers.Conv2DTranspose(1, 3, activation='relu')(x)\n",
        "\n",
        "autoencoder = keras.Model(encoder_input, decoder_output, name='autoencoder')\n",
        "autoencoder.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7FoQiMR2Niv",
        "colab_type": "text"
      },
      "source": [
        "Note that we make the decoding architecture strictly symmetrical to the encoding architecture, so that we get an output shape that is the same as the input shape `(28, 28, 1). The reverse of `Conv2D` layer is a `Conv2DTranspose` layer, and the reverse of `MaxPooling2D` layer is an `UpSampling2D` layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PEUBovt4tKb",
        "colab_type": "text"
      },
      "source": [
        "## All models are callable, just like layers\n",
        "You can treat any model as if it were a layer, by calling an `Input` or on the output of another layer. Note that by calling a model you aren't just reusing the architecutre of the model, you're also reusing its weights.\n",
        "\n",
        "Let's see this in action. Here's a different take on the autoencoder example that creates an encoder model, a decoder model, and a chain the in two calls to obtain the autoencoder model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYaB01Tq2Bbl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder_input = keras.Input(shape=(28, 28, 1), name='original_img')\n",
        "x = layers.Conv2D(16, 3, activation='relu')(encoder_input)\n",
        "x = layers.Conv2D(32, 3, activation='relu')(x)\n",
        "x = layers.MaxPooling2D(3)(x)\n",
        "x = layers.Conv2D(32, 3, activation='relu')(x)\n",
        "x = layers.Conv2D(16, 3, activation='relu')(x)\n",
        "encoder_output = layers.GlobalMaxPooling2D()(x)\n",
        "\n",
        "encoder = tf.keras.Model(inputs=encoder_input, outputs=encoder_output, name='encoder')\n",
        "encoder.summary()\n",
        "keras.utils.plot_model(encoder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6vIVDjK5jS2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoder_input = keras.Input(shape=(16,), name='encoding_img')\n",
        "x = layers.Reshape((4, 4, 1))(decoder_input)\n",
        "x = layers.Conv2DTranspose(16, 3, activation='relu')(x)\n",
        "x = layers.Conv2DTranspose(32, 3, activation='relu')(x)\n",
        "x = layers.UpSampling2D(3)(x)\n",
        "x = layers.Conv2DTranspose(16, 3, activation='relu')(x)\n",
        "decoder_output = layers.Conv2DTranspose(1, 3, activation='relu')(x)\n",
        "\n",
        "decoder = keras.Model(inputs=decoder_input, outputs=decoder_output, name='deocder')\n",
        "decoder.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBQ7Ogyi6yAX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "autoencoder_input = keras.Input(shape=(28, 28, 1), name='img')\n",
        "encoded_img = encoder(autoencoder_input)\n",
        "decoded_img = decoder(encoded_img)\n",
        "autoencoder = keras.Model(inputs=autoencoder_input, outputs=decoded_img, name='autoencoder')\n",
        "autoencoder.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIVZYbEy71lQ",
        "colab_type": "text"
      },
      "source": [
        "### Models are callable, just like layers!\n",
        "As you can see, model can be nested: a model can contain submodels (since a model is just like a layer)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNuDReWqLlyf",
        "colab_type": "text"
      },
      "source": [
        "A common use case formodel nesting is ensemblig. As an example, hre's how to ensemble a set of models into a single model that averages their predictions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4CR_wT67o-C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_model():\n",
        "  inputs = keras.Input(shape=(128, ))\n",
        "  outputs = layers.Dense(1, activation='sigmoid')(inputs)\n",
        "  return keras.Model(inputs, outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2b5FlrWML9CS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model1 = get_model()\n",
        "model2 = get_model()\n",
        "model3 = get_model()\n",
        "\n",
        "inputs = keras.Input(shape=(128, ))\n",
        "y1 = model1(inputs)\n",
        "y2 = model2(inputs)\n",
        "y3 = model3(inputs)\n",
        "outputs = layers.average([y1, y2, y3])\n",
        "ensemble_model = keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efgQ6uXvMVNU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ensemble_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9YeLmcbMXIS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "keras.utils.plot_model(ensemble_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NYEizEvMpAP",
        "colab_type": "text"
      },
      "source": [
        "## Manipulating complex graph topologies\n",
        "### Models with multiple inputs and outputs\n",
        "The functional API makes it easy to manipulate inputs and outputs. This cannot be handled with with the Sequential API.\n",
        "\n",
        "Here's a simple example.\n",
        "\n",
        "Let's say you're building a system for ranking custom issue tickets by priority and routing them to the right department.\n",
        "\n",
        "You model will have 3 inputs:\n",
        "- Title of the ticket (text input)\n",
        "- Text body of the ticket (text input)\n",
        "- Any tags added by the user (categorical input)\n",
        "It will have two outputs:\n",
        "- Priority score between 0 and 1 (scalar sigmoid output)\n",
        "- The department that should handle the ticket (softmax output over the set of departments)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFhL9QgLMhUj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_tags = 12 # Number of unique issue tags\n",
        "num_words = 10000 # Size of vocabulary obtained when preprocessing text data\n",
        "num_departments = 4 # Numboer of departments for predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAX7SWxMN5Yc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "title_input = keras.Input(shape=(None, ), name='title') # Variable-length sequence of ints\n",
        "body_input = keras.Input(shape=(None, ), name='body')  # Variable-length sequence of ints\n",
        "tags_input = keras.Input(shape=(num_tags, ), name='tags') # Binary vector of size `num_tags`"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7Ld3MbrORLS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Embed each word in the title into a 64-dimensional vector\n",
        "title_features = layers.Embedding(input_dim=num_words, output_dim=64)(title_input)\n",
        "# the largest integer (i.e. word index) in the input should be no larger\n",
        "# than 999 (vocabulary size).\n",
        "# Embed each word in the title into a 64-dimensional vector\n",
        "body_features = layers.Embedding(num_words, 64)(body_input)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-bUTqTQOzjO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reduce sequence of embedded words in the title into a single \n",
        "# 128-dimensional vector\n",
        "title_features = layers.LSTM(units=128)(title_features)\n",
        "# Reduce sequence of embedded words in the body into a single 32-dimensional vector\n",
        "body_features = layers.LSTM(32)(body_features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Mx7r-UBQMU8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Merge all available features into a single large vector via concatenation\n",
        "x = layers.concatenate([title_features, body_features, tags_input])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7QEeR-vQgMI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Stick a logistic regressing for priority prediction on top of the features\n",
        "priority_pred = layers.Dense(1, activation='sigmoid', name='priority')(x)\n",
        "# Stick a department classifier on top of the features\n",
        "department_pred = layers.Dense(num_departments, activation='softmax', name='department')(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFgKDNcaQzm8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Instantiate an end-to-end model predicting both priority and department\n",
        "model = keras.Model(inputs=[title_input, body_input, tags_input],\n",
        "                    outputs=[priority_pred, department_pred])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zzz-4NIVRB0M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "keras.utils.plot_model(model, 'multi_input_and_output_model.png', show_shapes=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNrkUvxBRuMW",
        "colab_type": "text"
      },
      "source": [
        "When compiling this model, we can assign different losses to each output. You can even assign differnet weights to each loss, to modulate their contribution to the total training loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jD-L0ceFRaWO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer=keras.optimizers.RMSprop(1e-3),\n",
        "              loss=['binary_crossentropy', 'categorical_crossentropy'],\n",
        "              loss_weights=[1., 0.2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aI87h8WUSJKb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Since we gave names to our output layers, we could also specify the loss like this:\n",
        "model.compile(optimizer=keras.optimizers.RMSprop(1e-3),\n",
        "              loss={'priority': 'binary_crossentropy',\n",
        "                    'department': 'categorical_crossentropy'},\n",
        "              loss_weights=[1., 0.2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIHATbUcScfU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We can train the model by passing lists of Numpy arrays of inputs and targets\n",
        "import numpy as np\n",
        "\n",
        "# Dummy input data\n",
        "title_data = np.random.randint(num_words, size=(1280, 10))\n",
        "body_data = np.random.randint(num_words, size=(1280, 10))\n",
        "tags_data = np.random.randint(2, size=(1280, num_tags)).astype('float32')\n",
        "\n",
        "# Dummy target data\n",
        "priority_targets = np.random.random(size=(1280, 1)) # Return random floats in the half-open interval [0.0, 1.0).\n",
        "dept_targets = np.random.randint(2, size=(1280, num_departments))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSn9BH5mT1zj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fit({'title': title_data, 'body': body_data, 'tags': tags_data},\n",
        "          {'priority': priority_targets, 'department': dept_targets},\n",
        "          epochs=2,\n",
        "          batch_size=32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FIKKz07UdNm",
        "colab_type": "text"
      },
      "source": [
        "When calling fit with a Dataset object, it should yield either a tuple of lists like ([title_data, body_data, tags_data], [priority_targets, dept_targets]) or a tuple of dictionaries like ({'title': title_data, 'body': body_data, 'tags': tags_data}, {'priority': priority_targets, 'department': dept_targets})."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbSCjwUvK23R",
        "colab_type": "text"
      },
      "source": [
        "## A toy resnet model\n",
        "In addition to models with multiple inputs and outputs, the Funtional API makes it easy to manipulate non-linear connectivity topologies, that is to say, models where layers are not connected sequentially. This also cannot be handled with the Sequential API (as the name indicates).\n",
        "\n",
        "A common use case for this is residual connections.\n",
        "\n",
        "Let's build a toy ResNet model for CIFAR10 to demonstrate this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHoIC8MsUHVQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = keras.Input(shape=(32, 32, 3), name='img')\n",
        "x = layers.Conv2D(32, 3, activation='relu')(inputs)\n",
        "x = layers.Conv2D(64, 3, activation='relu')(x)\n",
        "block_1_output = layers.MaxPooling2D(3)(x)\n",
        "\n",
        "x = layers.Conv2D(64, 3, activation='relu', padding='same')(block_1_output)\n",
        "x = layers.Conv2D(64, 3, activation='relu', padding='same')(x)\n",
        "block_2_output = layers.add([x, block_1_output])\n",
        "\n",
        "x = layers.Conv2D(64, 3, activation='relu', padding='same')(block_2_output)\n",
        "x = layers.Conv2D(64, 3, activation='relu', padding='same')(x)\n",
        "block_3_output = layers.add([x, block_2_output])\n",
        "\n",
        "x = layers.Conv2D(64, 3, activation='relu')(block_3_output)\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "x = layers.Dense(256, activation='relu')(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(10, activation='softmax')(x)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs, name='resnet')\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SP3bAXq3MPSN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "keras.utils.plot_model(model, 'resnet.png', show_shapes=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ht4wjneTMfFK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's train it\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "y_train = keras.utils.to_categorical(y_train, 10)\n",
        "y_test = keras.utils.to_categorical(y_test, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5RB3V89Ntpf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer=keras.optimizers.RMSprop(1e-3),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['acc'])\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=64,\n",
        "          epochs=10,\n",
        "          validation_split=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epgUJV0-Osmm",
        "colab_type": "text"
      },
      "source": [
        "## Sharing layers\n",
        "Another good use for the functional API are models that use shared layers. Shared layers are layer instances that get reused multiple times in a same model: they learn features that correspond to multiple paths in the graph-of-layers.\n",
        "\n",
        "Shared layers are often used to encode inputs that come from similar spaces (say, two different pieces of text that feature similar vocabulary), since they enable sharing of information across these different inputs, and they make it possible to train such a model on less data. If a given word is seen in one of the inputs, that will benefit the processing of all inputs that go through the shared layer.\n",
        "\n",
        "To share a layer in the Functional API, just call the same layer instance multiple times. For instance, here's an `Embedding` layer shared across two different text inputs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEjQjHb3N7Id",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Embedding for 1000 unique words mapped to 127-dimensional vectors\n",
        "shared_embedding = layers.Embedding(1000, 128)\n",
        "\n",
        "# Variable-length sequence of integers\n",
        "text_input_a = keras.Input(shape=(None, ), dtype='int32')\n",
        "\n",
        "# Variable-length sequence of integers\n",
        "text_input_b = keras.Input(shape=(None, ), dtype='int32')\n",
        "\n",
        "# We resue the same layer to encode both inputs\n",
        "encoded_input_a = shared_embedding(text_input_a)\n",
        "encoded_input_b = shared_embedding(text_input_b)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2d9ou5GQj-z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_a = keras.Model(text_input_a, encoded_input_a)\n",
        "model_b = keras.Model(text_input_b, encoded_input_b)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARLvEHuKQ7iz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "keras.utils.plot_model(model_a)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ct5NrFxXQ_Pi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "keras.utils.plot_model(model_b)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFiuGHLDRYTW",
        "colab_type": "text"
      },
      "source": [
        "## Extracting and reusing nodes in the graph of layers\n",
        "Because the graph of layers you are manipulating in the Functional API is a static datastructure, it can be accessed and inspected. This is how we are able to plot Functional models as images, for instance.\n",
        "\n",
        "This also means that we can access the activations of intermediate layers (\"nodes\" in the graph) and reuse them elsewhere. This is extremly useful for feature extraction , for example!\n",
        "\n",
        "Let's look at an example. This is a VGG19 modelwith weights pre-trained on ImageNet:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gKWpeQMRCif",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.applications import VGG19\n",
        "\n",
        "vgg19 = VGG19()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0D90qyulSRHP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vgg19.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XziSJ4UhSlT7",
        "colab_type": "text"
      },
      "source": [
        "And these are the intermediate activations of the mode, obtained by querying the graph datastructure:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33z2oP1TSfWX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vgg19.layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WgQMEQ3SzjS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features_list = [layer.output for layer in vgg19.layers] "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VI6-7hjSTSoc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4n_hy8BS5qi",
        "colab_type": "text"
      },
      "source": [
        "We can use these features to create a new feature-extraction model, that returns the values of the intermediate layer activations - and we ca do all of this in 3 lines."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DgRFcJwS3gc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feat_extraction_model = keras.Model(inputs=vgg19.input, outputs=features_list)\n",
        "\n",
        "img = np.random.random((1, 224,224, 3)).astype('float32')\n",
        "extracted_features = feat_extraction_model(img)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjpjrIgoUBfr",
        "colab_type": "text"
      },
      "source": [
        "## Extending the API by writing custom layers\n",
        "tf.keras has a wide range of built-in layers. Here are a few examples:\n",
        "- Convolutional layers: `Conv1D`, `Conv2D`, `Conv3D`, `Conv2DTranspose`, etc\n",
        "- Pooling layers: `MaxPooling1D`, `MaxPooling2D`, `MaxPooling3D`, `AveragePooling1D`, etc.\n",
        "- RNN layers: `GRU`, `LSTM`, `ConvLSTM2D`, etc.\n",
        "- `BatchNormalization`, `Dropout`, Embedding`, etc.\n",
        "\n",
        "If you don't find what you need, it's easy to extend the API by creating your own layers.\n",
        "\n",
        "All layers subclass the `Layer` class and implement:\n",
        "- A `call` method, that specifies the computation don by the layer.\n",
        "- A `build` method, that creates the weights of the layer(note that this is just a style convention; you could create weights in `__init__` as well)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWw1wCEhT2ZJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Simple implementation of a Dense layer\n",
        "class CustomDense(layers.Layer):\n",
        "  def __init__(self, units=32):\n",
        "    super(CustomDense, self).__init__()\n",
        "    self.units = units\n",
        "  \n",
        "  def build(self, input_shape):\n",
        "    self.w = self.add_weight(shape=(input_shape[-1], self.units),\n",
        "                             initializer='random_normal',\n",
        "                             trainable=True)\n",
        "    self.b = self.add_weight(shape=(self.units, ),\n",
        "                             initializer='random_normal',\n",
        "                             trainable=True)\n",
        "    \n",
        "  def call(self, inputs):\n",
        "    return tf.matmul(inputs, self.w) + self.b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ca-1506PWEmV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = keras.Input(shape=(4, ))\n",
        "outputs = CustomDense(10)(inputs)\n",
        "\n",
        "model = keras.Model(inputs, outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlF4X4f3W8kw",
        "colab_type": "text"
      },
      "source": [
        "If you want your custom layer to support serialization, you should also define a `get_config` method, that returns the constructor arguments of the layer instance:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYxNlOUQW4qE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomDense(layers.Layer):\n",
        "  def __init__(self, units=32):\n",
        "    super(CustomDense, self).__init__()\n",
        "    self.units = units\n",
        "  \n",
        "  def build(self, input_shape):\n",
        "    self.w = self.add_weight(shape=(input_shape[-1], self.units),\n",
        "                              initializer='random_normal',\n",
        "                              trainable=True)\n",
        "    self.b = self.add_weight(shape=(self.units, ),\n",
        "                              initializer='random_normal',\n",
        "                              trainable=True)\n",
        "    \n",
        "  def call(self, inputs):\n",
        "    return tf.matmul(inputs, self.w) + self.b\n",
        "  \n",
        "  def get_config(self):\n",
        "    return {'units': self.units}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oD-AgKIwYBxa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = keras.Input((4, ))\n",
        "outputs = CustomDense(10)(inputs)\n",
        "\n",
        "model = keras.Model(inputs, outputs)\n",
        "config = model.get_config()\n",
        "\n",
        "new_model = keras.Model.from_config(\n",
        "    config, custom_objects={'CustomDense': CustomDense})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSnqpdTcYtIO",
        "colab_type": "text"
      },
      "source": [
        "Optionally, you could also implement the classmethod `from_config(cls, config)`, which is in charge of recreating a layer instance given its config directory. Ths default implementation of `from_comfig` is:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ynR1LuBYST9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def from_config(cls, config):\n",
        "  return cls(**config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mO67WP9ZlVj",
        "colab_type": "text"
      },
      "source": [
        "## When to use the Functional API\n",
        "How to decide whether to use Functinoal API to create a new model, or just subclass the `Model` class directly?\n",
        "\n",
        "In general, the Functional API is higher-level, easier&safer to use, and has a number of features that subclassed Models do not support.\n",
        "\n",
        "However, Model subclassing gives you greater flexibility when creating models that are not easily expressible as directed acyclic graphs of layers (for instance, you could not implement a Tree-RNN with Functional API, you would have to subclass `Model` directly)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BerW0YPYazpR",
        "colab_type": "text"
      },
      "source": [
        "## Here are the strengths of the Functional API:\n",
        "The properties listed below are all true for Sequential models as well (which are also data structures), but they aren't true for subclassed models (which are Python bytecode, not data structures)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j22D-kTOfjLl",
        "colab_type": "text"
      },
      "source": [
        "**It is less verbose.\n",
        "No `super(MyClass, self).__init__(...),\n",
        "No def call(self, ...):** etc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRFUvxd3Y6aa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compare:\n",
        "inputs = keras.Input(shape=(32, ))\n",
        "x = layers.Dense(64, activation='relu')(inputs)\n",
        "outputs = layers.Dense(10)(x)\n",
        "mlt = keras.Model(inputs, outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOB5hrDPbibu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# With the subclassed version:\n",
        "class MLP(keras.Model):\n",
        "  def __init__(self, **kwargs):\n",
        "    super(MLP, self).__init__(**kwargs)\n",
        "    self.dense_1 = layers.Dense(64, activation='relu')\n",
        "    self.dense_2 = layers.Dense(10)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = self.dense_1(inputs)\n",
        "    return self.dense_2(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lw7yxK-sb4lY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Instantiate the model.\n",
        "mlp = MLP()\n",
        "# Necessary to create the model's state.\n",
        "# The model doesn't have a state until it's called at least once.\n",
        "_ = mlp(tf.zeros((1, 32)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yrlr9Ue0fgWy",
        "colab_type": "text"
      },
      "source": [
        "**It validates your model while you're defining it.**\n",
        "In the Functional API, your input specification (shape and dtype) is created in advance (via `Input`), and every time you call a layer, the layer checks that the specification passed to it matches its assumptions, and it will raise a helpful error message if not.\n",
        "\n",
        "This guarantess that any model you can build with the Functional API will run. All debugging (other than convergence-related debugging) will happen **statically during the model construction, and not at execution time**. This is similar to typechecking in a compiler."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-pK_i-fg6zv",
        "colab_type": "text"
      },
      "source": [
        "**Your Functional model is plottable and inspectable.**\n",
        "You can plot the model as a graph, and you can easily intermediate nodes in this graph-for instance, to extract and reuer the activations of intermediate layers, as we saw in the previous example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEcdCIZshOei",
        "colab_type": "text"
      },
      "source": [
        "**Your Functional model can be serialized or cloned.**\n",
        "Because a Functional model is a data structure rather than a piece of code, it is safely serializable and can be saved as a single file that allows you to recreate the exact same model without having access to any of the original code. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2xtVuXUh8rY",
        "colab_type": "text"
      },
      "source": [
        "## Here are the weaknesses of the Functional API\n",
        "**It does not support dynamic architectures.**\n",
        "The Functional API treats model as DAGs of layers. This is true for the most deep learning architectures, but not all: for instance, recursive networks or Tree RNNs do not follow this assumption and cannot be implemented in the Functional API."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2HKh-fziWMf",
        "colab_type": "text"
      },
      "source": [
        "**Sometimes, you just need to write everything from scratch.**\n",
        "When writing advanced architectures, you may want to do things that are outside the scope of \"defining a DAG of layers\": for instance, you may want to expose multiple custom training and inference methods on your model instance. This requires subclassing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlJriiqmmvQU",
        "colab_type": "text"
      },
      "source": [
        "## Mix-and-matching different API styles\n",
        "Importantly, choosing between the Functional API or Model subclassing isn't binary decision that restricts you to one category of models. All models in the tf.keras API can interact with each, whether they're Sequential models, Functional models, or subclassed Models/Layers written from scratch.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Iw2CISscGc7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# You can always use a Functional model or Sequential model as part of a subclassed Model/Layer:\n",
        "units = 32\n",
        "timesteps = 10\n",
        "input_dim = 5\n",
        "\n",
        "# Define a Functional model\n",
        "inputs = keras.Input((None, units))\n",
        "x = layers.GlobalAveragePooling1D()(inputs)\n",
        "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
        "model = keras.Model(inputs, outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeWOXdZWnocA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDIz1cbXntRR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomRNN(layers.Layer):\n",
        "  def __init__(self):\n",
        "    super(CustomRNN, self).__init__()\n",
        "    self.units = units\n",
        "    self.projection_1 = layers.Dense(units=units, activation='tanh')\n",
        "    self.projection_2 = layers.Dense(units=units, activation='tanh')\n",
        "    # Our previously-definded Functional model\n",
        "    self.classifier = model\n",
        "  \n",
        "  def call(self, inputs):\n",
        "    outputs = []\n",
        "    state = tf.zeros(shape=(inputs.shape[0], self.units))\n",
        "    for t in range(inputs.shape[1]):\n",
        "      x = inputs[:, t, :]\n",
        "      h = self.projection_1(x)\n",
        "      y = h + self.projection_2(state)\n",
        "      state = y\n",
        "      outputs.append(y)\n",
        "    features = tf.stack(outputs, axis=1)\n",
        "    print(features.shape)\n",
        "    return self.classifier(features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2u_7viWocqO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rnn_model = CustomRNN()\n",
        "_ = rnn_model(tf.zeros((1, timesteps, input_dim)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5dNl54cooP6",
        "colab_type": "text"
      },
      "source": [
        "Inversely, you can use any subclassed Lyaer or Model in the Functional API as long as it implements a `call` method that follows one of the following patterns:\n",
        "- `call(self, inputs, **kwargs)` where `inputs` is a tensor or a nested structure of tensor (e.g. a list of tensors), and where `**kwargs` are non-tensor arguments (non-inputs).\n",
        "- `call(self, inputs, training=None, **kwargs)` where `training` is a boolean indicating whether the layer should behave un training mode and inference mode.\n",
        "- `call(self, inputs, mask=None, **kwargs)` where `mask` is a boolean mask tensor (useful for RNNs for instance).\n",
        "- `call (self, inputs, training=None, mask=None, **kwargs)`\n",
        "In addition, if you implement the `get_config` method on your custom Lyaer or Mode, the Funcitonal models you create with it will still be serializable and clonable.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FL0ZdnGojYQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# RNN written from scratch in Functional model:\n",
        "units = 32\n",
        "timesteps = 10\n",
        "input_dim = 5\n",
        "batch_size = 16\n",
        "\n",
        "class CustomRNN(layers.Layer):\n",
        "  def __init__(self):\n",
        "    super(CustomRNN, self).__init__()\n",
        "    self.units = units\n",
        "    self.projection_1 = layers.Dense(units=units, activation='tanh')\n",
        "    self.projection_2 = layers.Dense(units=units, activation='tanh')\n",
        "    self.classifier = layers.Dense(1, activation='sigmoid')\n",
        "  \n",
        "  def call(self, inputs):\n",
        "    outputs = []\n",
        "    state = tf.zeros(shape=(inputs.shape[0], self.units))\n",
        "    for t in range(inputs.shape[1]):\n",
        "      x = inputs[:, t, :]\n",
        "      h = self.projection_1(x)\n",
        "      y = h + self.projection_2(state)\n",
        "      state = y\n",
        "      outputs.append(y)\n",
        "    features = tf.stack(outputs, axis=1)\n",
        "    return self.classifier(features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKfJqE4ZqiVn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Note that we specify a static batch_size for the inputs with the\n",
        "# `batch_shape` args, because the inner computation of `CustomRNN` requires \n",
        "# a static batch size (when we create the `state` zeros tensor)\n",
        "inputs = keras.Input(batch_shape=(batch_size, timesteps, input_dim))\n",
        "x = layers.Conv1D(32, 3)(inputs)\n",
        "outputs = CustomRNN()(x)\n",
        "\n",
        "rnn_model = keras.Model(inputs, outputs)\n",
        "_ = rnn_model(tf.zeros((1, 10, 5)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXnWtWOnrFa7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now you have at your fingertips a powerful set of tools for building deep learning models."
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}