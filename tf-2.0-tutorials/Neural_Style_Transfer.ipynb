{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural_Style_Transfer.ipynb",
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B18kYgJtUMlL",
        "colab_type": "text"
      },
      "source": [
        "This tutorial uses deep learning to compose one image in the style of another image (ever wish you could paint like Picasso or Van Gogh?). This is known as `neural style transfer` and the technique is outlined in `A Neural Algorithm of Artistic Style`\n",
        "\n",
        "Neural style transfer is an optization used to take two images-a `content` image and `style reference` image (such as an artwork by a famous painter)-and blend them together so the output image looks like the content image, bug \"painter\" in the style of the style reference image.\n",
        "\n",
        "This is implemented by optimizing the output image to match the content statistics of the content image and style statistics of the style reference image. There statistics are extracted from the images using a convolutional network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CYPtR05T2Tx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJGuY2VL7v-j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tf-nightly-2.0-preview\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bb2oyq_a7y1j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import IPython.display as display\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "mpl.rcParams['figure.figsize'] = (12, 12)\n",
        "mpl.rcParams['axes.grid'] = False\n",
        "\n",
        "import numpy as np\n",
        "import time\n",
        "import functools"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MljA2htv8NJv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "content_path = tf.keras.utils.get_file('turtle.jpg','https://storage.googleapis.com/download.tensorflow.org/example_images/Green_Sea_Turtle_grazing_seagrass.jpg')\n",
        "style_path = tf.keras.utils.get_file('kandinsky.jpg','https://storage.googleapis.com/download.tensorflow.org/example_images/Vassily_Kandinsky%2C_1913_-_Composition_7.jpg')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vioAyAO8fBC",
        "colab_type": "text"
      },
      "source": [
        "### Visualize the input\n",
        "Define a function to load an image and limit its maximum dimension to 512 pixels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_kXjSLY8aim",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_img(path_to_img):\n",
        "  max_dim = 512\n",
        "  img = tf.io.read_file(path_to_img)\n",
        "  img = tf.image.decode_image(img, channels=3)\n",
        "  img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "\n",
        "  shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n",
        "  long_dim = max(shape)\n",
        "  scale = max_dim / long_dim\n",
        "\n",
        "  new_shape = tf.cast(shape * scale, tf.int32)\n",
        "\n",
        "  img = tf.image.resize(img, new_shape)\n",
        "  img = img[tf.newaxis, :]\n",
        "  return img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJU0bW2F9piK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def imshow(image, title=None):\n",
        "  if len(image.shape) > 3:\n",
        "    image = tf.squeeze(image, axis=0)\n",
        "  \n",
        "  plt.imshow(image)\n",
        "  if title:\n",
        "    plt.title(title)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCFtKQqZ-NX0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "content_image = load_img(content_path)\n",
        "style_image = load_img(style_path)\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "imshow(content_image, 'Content Image')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "imshow(style_image, 'Style Image')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ganLS1q_-iJj",
        "colab_type": "text"
      },
      "source": [
        "### Define content and style representations\n",
        "\n",
        "Use the intermediate layers of the model to get the `content` and `style` representations of the image. Starting from the network's input layer, the first few layer activations represent low-level features like edges and textures. As you step through the network, the final few layers represent higher-level features-object parts like wheels or eyes. In this case, you are using the VGG19 network architecture, a pretrained image classification network. These intermediate layers are necessary to define the representation of content and style from the images. For an input image, try to match the corresponding style and content target representations at these intermediate layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tvn4B8Fo-ce_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load a VGG19 and test run it on our image to ensure it's used correctly:\n",
        "x = tf.keras.applications.vgg19.preprocess_input(content_image*255)\n",
        "x = tf.image.resize(x, (224, 224))\n",
        "vgg = tf.keras.applications.VGG19(include_top=True, weights='imagenet')\n",
        "prediction_probabilities = vgg(x)\n",
        "prediction_probabilities.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQYok9fbAhx-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "content_image.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uo4VXpJApRW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.image.resize(content_image, (224, 224)).shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCsngOiEA0i7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted_top_5 = tf.keras.applications.vgg19.decode_predictions(\n",
        "      prediction_probabilities.numpy()\n",
        ")[0]\n",
        "[(class_name, prob) for (number, class_name, prob) in predicted_top_5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6JT_oKuBVlT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load a `VGG19` without the classification head, and list the layer names\n",
        "vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n",
        "\n",
        "print()\n",
        "for layer in vgg.layers:\n",
        "  print(layer.name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTXoz5X1B6Lk",
        "colab_type": "text"
      },
      "source": [
        "Choose intermediate layers from the network to represent the style and content of the image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INx9R7RQB0gu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Content layer where will pull our feature maps\n",
        "# feature map: output of one filter applied\n",
        "content_layers = ['block5_conv2']\n",
        "\n",
        "# Style layer of interest\n",
        "style_layers = ['block1_conv1',\n",
        "                'block2_conv1',\n",
        "                'block3_conv1',\n",
        "                'block4_conv1',\n",
        "                'block5_conv1']\n",
        "num_content_layers = len(content_layers)\n",
        "num_style_layers = len(style_layers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-Kzj_hdDAwl",
        "colab_type": "text"
      },
      "source": [
        "### Intermediate layers for style and content\n",
        "So why do these intermediate outputs within our pretrained image classification network allow us to define style and content representations?\n",
        "\n",
        "At a high level, in order for a network to perform image classification (which this network has been trained to do), it must understand the image. This requires taking the raw image as input pixels and building an internal representation that converts the raw image pixels into a complex understanding of the features present within the image.\n",
        "\n",
        "This is also a reason why convolutional neural networks are able to generalize well: they're able to capture the invariances and defining features within classes that are agnostic to background noise and other nuisances. Thus, somewhere between where the raw image is fed into the model and the output classification label, the model serves as a complex feature extractor. By accessing intermediate layers of the model, you're able to describe the content and style of input images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmuTVb8oCkBG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def vgg_layers(layer_names):\n",
        "  \"\"\" Creates a vgg model that returns a list of intermediate output values.\"\"\"\n",
        "  # Load our model. Load pretrained VGG, trained on imagenet data\n",
        "  vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n",
        "  vgg.trainable=False\n",
        "\n",
        "  outputs = [vgg.get_layer(name).output for name in layer_names]\n",
        "\n",
        "  model = tf.keras.Model([vgg.input], outputs)\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJigr9G9Et3q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "style_extractor = vgg_layers(style_layers)\n",
        "style_outputs = style_extractor(style_image*255)\n",
        "\n",
        "# Look at the statistics of each layer's output\n",
        "for name, output in zip(style_layers, style_outputs):\n",
        "  print(name)\n",
        "  print(\"  shape: \", output.numpy().shape)\n",
        "  print(\"  min: \", output.numpy().min())\n",
        "  print(\"  max: \", output.numpy().max())\n",
        "  print(\"  mean: \", output.numpy().mean())\n",
        "  print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0M9LTsuLGO5x",
        "colab_type": "text"
      },
      "source": [
        "### Calculate style\n",
        "The content of an image is represented by the values of the intermediate feature maps.\n",
        "\n",
        "It turns out, the style of an image can be described by the means and correlations across the different maps. Calculate a Gram matrix that includes this information by taking the outer product of the feature vector with itself at each location, and averaging that outer product over all locations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8mW7PcTFV0U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gram_matrix(input_tensor):\n",
        "  result = tf.linalg.einsum('bijc,bijd->bcd', input_tensor, input_tensor)\n",
        "  input_shape = tf.shape(input_tensor)\n",
        "  num_locations = tf.cast(input_shape[1]*input_shape[2], tf.float32)\n",
        "  return result/(num_locations)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Q4PuygoHcO1",
        "colab_type": "text"
      },
      "source": [
        "### Extract style and content"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CvrU7xlHaw3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class StyleContentModel(tf.keras.models.Model):\n",
        "  def __init__(self, style_layers, content_layers):\n",
        "    super(StyleContentModel, self).__init__()\n",
        "    self.vgg = vgg_layers(style_layers + content_layers)\n",
        "    self.style_layers = style_layers\n",
        "    self.content_layers = content_layers\n",
        "    self.num_style_layers = len(style_layers)\n",
        "    self.vgg.trainable = False\n",
        "\n",
        "  def call(self, inputs):\n",
        "    # Expect float input in [0, 1]\"\n",
        "    inputs = inputs * 255.0\n",
        "    preprocessed_input = tf.keras.applications.vgg19.preprocess_input(inputs)\n",
        "    outputs = self.vgg(preprocessed_input)\n",
        "    style_outputs, content_outputs = (outputs[:self.num_style_layers],\n",
        "                                      outputs[self.num_style_layers:])\n",
        "    \n",
        "    style_outputs = [gram_matrix(style_output)\n",
        "                     for style_output in style_outputs]\n",
        "\n",
        "    content_dict = {content_name:value\n",
        "                    for content_name, value\n",
        "                    in zip(self.content_layers, content_outputs)}\n",
        "    \n",
        "    style_dict = {style_name:value\n",
        "                  for style_name, value\n",
        "                  in zip(self.style_layers, style_outputs)}\n",
        "    \n",
        "    return {'content':content_dict, 'style':style_dict}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUS9N-naJDY1",
        "colab_type": "text"
      },
      "source": [
        "When called on an image, this model returns the gram matrix (style) of the `style_layers` and content of `content_layers`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Em1HM3JeJA0Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "extractor = StyleContentModel(style_layers, content_layers)\n",
        "\n",
        "results = extractor(tf.constant(content_image))\n",
        "\n",
        "style_results = results['style']\n",
        "\n",
        "print('Styles:')\n",
        "for name, output in sorted(results['style'].items()):\n",
        "  print(\"  \", name)\n",
        "  print(\"    shape: \", output.numpy().shape)\n",
        "  print(\"    min: \", output.numpy().min())\n",
        "  print(\"    max: \", output.numpy().max())\n",
        "  print(\"    mean: \", output.numpy().mean())\n",
        "  print()\n",
        "\n",
        "print(\"Contents:\")\n",
        "for name, output in sorted(results['content'].items()):\n",
        "  print(\"  \", name)\n",
        "  print(\"    shape: \", output.numpy().shape)\n",
        "  print(\"    min: \", output.numpy().min())\n",
        "  print(\"    max: \", output.numpy().max())\n",
        "  print(\"    mean: \", output.numpy().mean())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sa22YsMVJ58p",
        "colab_type": "text"
      },
      "source": [
        "### Run gradient descent\n",
        "With this style and content extractor, you can now implement the style transfer algorithm. Do this by calculating the mean squared error for your image's output relative to each target, then take the weighed sum of these losses.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaR25EfOJm-7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "style_targets = extractor(style_image)['style']\n",
        "content_targets = extractor(content_image)['content']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5Vuxv6qKktT",
        "colab_type": "text"
      },
      "source": [
        "Define a `tf.Variable` to contain image to optimize. To make this quick, initialize it with the content image (the `tf.Variable` must be the same shape as the content image):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4umMtRDbKtjm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image = tf.Variable(content_image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekH11JP2LASL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Since this is a float image, define a function to keep the pixel values between 0 and 1:\n",
        "def clip_0_1(image):\n",
        "  return tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYZaUS--Lhsh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create an optimizer.\n",
        "# The paper recommends LBFGS, but Adam works okay too.\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=0.02, beta_1=0.99, epsilon=1e-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgrPDIHcLx3k",
        "colab_type": "text"
      },
      "source": [
        "To optimize this, use a weighted combination of the two losses to get the total loss:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7R3FfqhLvdd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "style_weight = 1e-2\n",
        "content_weight = 1e4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwvWanWFQUq9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def style_content_loss(outputs):\n",
        "    style_outputs = outputs['style']\n",
        "    content_outputs = outputs['content']\n",
        "    style_loss = tf.add_n([tf.reduce_mean((style_outputs[name]-style_targets[name])**2) \n",
        "                           for name in style_outputs.keys()])\n",
        "    style_loss *= style_weight / num_style_layers\n",
        "\n",
        "    content_loss = tf.add_n([tf.reduce_mean((content_outputs[name]-content_targets[name])**2) \n",
        "                             for name in content_outputs.keys()])\n",
        "    content_loss *= content_weight / num_content_layers\n",
        "    loss = style_loss + content_loss\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4e2fSg3QVA4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function()\n",
        "def train_step(image):\n",
        "  with tf.GradientTape() as tape:\n",
        "    outputs = extractor(image)\n",
        "    loss = style_content_loss(outputs)\n",
        "\n",
        "  grad = tape.gradient(loss, image)\n",
        "  opt.apply_gradients([(grad, image)])\n",
        "  image.assign(clip_0_1(image))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tiu0P30mQWX0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_step(image)\n",
        "train_step(image)\n",
        "train_step(image)\n",
        "plt.imshow(image.read_value()[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSqM5lZ4QdF5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Perform a longer optimization:\n",
        "import time\n",
        "\n",
        "tic = time.time()\n",
        "epochs = 5\n",
        "steps_per_epoch = 10\n",
        "\n",
        "step = 0\n",
        "for n in range(epochs):\n",
        "  for m in range(steps_per_epoch):\n",
        "    step += 1\n",
        "    train_step(image)\n",
        "    print('.', end='')\n",
        "  display.clear_output(wait=True)\n",
        "  imshow(image.read_value())\n",
        "  plt.title('Train step: {}'.format(step))\n",
        "  plt.show()\n",
        "\n",
        "toc = time.time()\n",
        "print('Total time: {:.1f}'.format(toc - tic))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruhz-CmYS9ia",
        "colab_type": "text"
      },
      "source": [
        "# Total variation loss\n",
        "One downside to this basic implementation is that it produces a lot of high frequency artifacts. Decrease these using an explicit regularization term on the high frequency components of the image. In style transfer, this is often called the total variation loss:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tTzv3-XT6xP",
        "colab_type": "text"
      },
      "source": [
        "low frequencies in images mean pixel values that are changing slowly over space, while high frequency content means pixel values that are rapidly changing in space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jgVaAxaRUXl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def high_pass_x_y(image):\n",
        "  x_var = image[:,:,1:,:] - image[:,:,:-1,:]\n",
        "  y_var = image[:,1:,:,:] - image[:,:-1,:,:]\n",
        "\n",
        "  return x_var, y_var"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtTvstYiT7qA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_deltas, y_deltas = high_pass_x_y(content_image)\n",
        "\n",
        "plt.figure(figsize=(14, 10))\n",
        "plt.subplot(2, 2, 1)\n",
        "imshow(clip_0_1(2*y_deltas+0.5), 'Horizontal Deltas: Original')\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "imshow(clip_0_1(2*x_deltas+0.5), 'Vertical Deltas: Original')\n",
        "\n",
        "x_deltas, y_deltas = high_pass_x_y(image)\n",
        "\n",
        "plt.subplot(2,2,3)\n",
        "imshow(clip_0_1(2*y_deltas+0.5), \"Horizontal Deltas: Styled\")\n",
        "\n",
        "plt.subplot(2,2,4)\n",
        "imshow(clip_0_1(2*x_deltas+0.5), \"Vertical Deltas: Styled\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPFBl3AMUudg",
        "colab_type": "text"
      },
      "source": [
        "This shows how to high frequency components have increased."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuS-DOCdUzhh",
        "colab_type": "text"
      },
      "source": [
        "Also, this high frequency component is basically an edge-detector. You can get similar output from the Sobel edge detector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7DoxMX_UnG0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(14,10))\n",
        "\n",
        "sobel = tf.image.sobel_edges(content_image)\n",
        "plt.subplot(1,2,1)\n",
        "imshow(clip_0_1(sobel[...,0]/4+0.5), \"Horizontal Sobel-edges\")\n",
        "plt.subplot(1,2,2)\n",
        "imshow(clip_0_1(sobel[...,1]/4+0.5), \"Vertical Sobel-edges\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d7qv4w-VPMK",
        "colab_type": "text"
      },
      "source": [
        "The regularization loss associated with this is the sum of the squares of the values:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0_BVMVwVHgN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def total_variation_loss(image):\n",
        "  x_deltas, y_deltas = high_pass_x_y(image)\n",
        "  return tf.reduce_mean(x_deltas**2) + tf.reduce_mean(y_deltas**2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ue41QVEOWjVm",
        "colab_type": "text"
      },
      "source": [
        "### Re-run the optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOAoeZaUWZbC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Choose a weight for the `total_variation_loss`\n",
        "total_variation_weight = 1e8"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQzYQoi0WwlJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function()\n",
        "def train_step(image):\n",
        "  with tf.GradientTape() as tape:\n",
        "    outputs = extractor(image)\n",
        "    loss = style_content_loss(outputs)\n",
        "    loss += total_variation_weight * total_variation_loss(image)\n",
        "\n",
        "  grad = tape.gradient(loss, image)\n",
        "  opt.apply_gradients([(grad, image)])\n",
        "  image.assign(clip_0_1(image))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11sYa-e2XKwc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reinitialize the optimization variable\n",
        "image = tf.Variable(content_image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i73HEdNQYFeN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run the optimization\n",
        "import time\n",
        "tic = time.time()\n",
        "\n",
        "epochs = 5\n",
        "steps_per_epoch = 10\n",
        "\n",
        "step = 0\n",
        "for n in range(epochs):\n",
        "  for m in range(steps_per_epoch):\n",
        "    step += 1\n",
        "    train_step(image)\n",
        "    print(\".\", end='')\n",
        "  display.clear_output(wait=True)\n",
        "  imshow(image.read_value())\n",
        "  plt.title(\"Train step: {}\".format(step))\n",
        "  plt.show()\n",
        "\n",
        "toc = time.time()\n",
        "print(\"Total time: {:.1f}\".format(toc - tic))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}