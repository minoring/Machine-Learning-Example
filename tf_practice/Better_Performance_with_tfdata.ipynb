{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Better_Performance_with_tfdata.ipynb",
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmTP2YwMo0Dm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtx9SeEWpwFm",
        "colab_type": "text"
      },
      "source": [
        "Making reproducible performance benchmarks can be difficults, different factors impacting it:\n",
        "- the current CPU load,\n",
        "- the network traffic,\n",
        "- complex mechanisms like cache, etc.\n",
        "Hence, provide a reproducible benchmark, build an artificial example.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIMqX_xMp-KG",
        "colab_type": "text"
      },
      "source": [
        "Define a class inheriting from `tf.data.Dataset` called `ArtificialDataset`,\n",
        "This dataset:\n",
        "- Generates `num_samples` samples (default is 3)\n",
        "- Sleeps for some time before the first item to simulate opening a file\n",
        "- Sleeps for some time before producing each item to simulate reading data from a file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_C0EUo6fpWfT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ArtificialDataset(tf.data.Dataset):\n",
        "  def _generator(num_samples):\n",
        "    # Opening the file\n",
        "    time.sleep(0.03)\n",
        "\n",
        "    for sample_idx in range(num_samples):\n",
        "      # Reading data (line, record) from the file\n",
        "      time.sleep(0.015)\n",
        "\n",
        "      yield(sample_idx, )\n",
        "  \n",
        "  def __new__(cls, num_samples=3):\n",
        "    return tf.data.Dataset.from_generator(\n",
        "        cls._generator,\n",
        "        output_types=tf.dtypes.int64,\n",
        "        output_shapes=(1, ),\n",
        "        args=(num_samples, ))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9mWSlTzrGKm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def benchmark(dataset, num_epochs=2):\n",
        "  \"\"\"Write a dummy training loop that measures how long it takes to iterate over a dataset.\n",
        "  Training time is simulated.\"\"\"\n",
        "  start_time = time.perf_counter()\n",
        "  for epoch_num in range(num_epochs):\n",
        "    for sample in dataset:\n",
        "      # Performing a training step\n",
        "      time.sleep(0.01)\n",
        "  tf.print('Execution time:', time.perf_counter() - start_time)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXXdolOArv-4",
        "colab_type": "text"
      },
      "source": [
        "## Optimize performance\n",
        "To exhibit how performance can be optimized, you will improve the performance of the `ArtificialDataset`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXGsQ_A1r5R9",
        "colab_type": "text"
      },
      "source": [
        "### The naive approach\n",
        "Start with a naive pipeline using no tricks, iterating over the dataset as-is."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mY2BFoJrqgn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "benchmark(ArtificialDataset())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJPE5Erps4b2",
        "colab_type": "text"
      },
      "source": [
        "### Prefetching\n",
        "Prefetching overlaps the processing and model execution of a training step. While the model is executing training step `s`, the input pipeline is reading the data for step `s+1`. Doing so reduces the step time to the maximum (as opposed to the sum) of the training and the time it takes to extract the data.\n",
        "\n",
        "The `tf.data` API provides the `tf.data.Dataset.prefetch` transformation. It can be used to decouple the time when data is produced from the time when data is consumed. In particular, the transformation uses a background thread and an internal buffer to prefetch elements from the input dataset ahead of the time they are requested. The number of elements to prefetch should be equal to (or possibly greater than) the number of batches consumed by a single training step. You could either manually tune this value, or set it to `tf.data.experimental.AUTOTUNE` which will prompt the `tf.data` runtime to tune the value dynamically at runtime.\n",
        "\n",
        "Note that the prefetch transformation provides benefits any time there is opportunity to overlap the work of a \"producer\" with the work of a \"consumer\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeBzrj0rsGAJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "benchmark(\n",
        "    ArtificialDataset()\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovsGBRSox2QN",
        "colab_type": "text"
      },
      "source": [
        "### Parallelizing data extraction\n",
        "In a real-world setting, the input data may be stored remotely (for example, GCS or HDFS). A dataset pipeline that works well when reading data locally might become bottlenecked on I/O when reading data remotely because of the following differences between local and remote storate:\n",
        "- **Time-to-first-byte**: Reading the first byte of a file from remote storate can take orders of magnitude longer than from local storate.\n",
        "- **Read throughput**: While remote storate typically offers large aggregate bandwidth, reading a single file might only be able to utilize a small fraction of this bandwidth.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEN4Fi4izxYt",
        "colab_type": "text"
      },
      "source": [
        "### Sequential interleave\n",
        "The default arguments of the `tf.data.Dataset.interleave` transformation make it interleave single samples from two datasets sequentially."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5POLkSuKvrfg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Feching samples alternatively from the two datasets available.\n",
        "# No performance improvement is involved here.\n",
        "benchmark(\n",
        "    tf.data.Dataset.range(2)\n",
        "    .interleave(ArtificialDataset)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5WmNnWL0QV7",
        "colab_type": "text"
      },
      "source": [
        "### Parallel interleave\n",
        "="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwenVY3o0H6Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now use the `num_parallel_calls` argument of the `interleave` transformation. This loads\n",
        "# multiple datasets in parallel, reducing the time waiting for the files to be opened.\n",
        "benchmark(\n",
        "    tf.data.Dataset.range(2)\n",
        "    .interleave(\n",
        "        ArtificialDataset,\n",
        "        num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
        "    )\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0RoSYEeKjZ5",
        "colab_type": "text"
      },
      "source": [
        "### Parallelizing data transformation\n",
        "When preparing data, input elements may need to be pre-processed. To this end, the `tf.data` API offers the `tf.data.Dataset.map` transformation, which applies a user-defined function to each element of the input dataset. Because input elements are independent of one another, the pre-processing can be parallelized across multiple CPI cores. To make this possible, similarly to the `prefetch` and `interleave` transformations, the `map` transformation provides the `num_parallel_calls` argument to specify the level of parallelism.\n",
        "\n",
        "Choosing the best value for the `num_parallel_calls` argument depends on your hardware, characteristics of your training data (such as its size and shape), the cost of your map function, and what other processing is happening on the CPU at the same time. A simple heuristic is to use the number of available CPU cores. However, as for the `prefetch` and `interleave` transformation supports, `tf.data.experimental.AUTOTUNE` which will delegate the decision about what level of parallelism to use to the `tf.data` runtime."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7l5ObkC0wB3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mapped_function(s):\n",
        "  # Do some hard pre-processing\n",
        "  tf.py_function(lambda: time.sleep(0.03), [], ())\n",
        "  return s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_JMMPgqKjK5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sequential mapping\n",
        "# Start by using the `map` transformation without parallelism as a baseline example\n",
        "benchmark(\n",
        "    ArtificialDataset()\n",
        "    .map(mapped_function)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRTR-hdxLzzp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Parallel mapping\n",
        "# Now, use the same pre-processing function but apply it in parallel on multiple samples.\n",
        "benchmark(\n",
        "    ArtificialDataset()\n",
        "    .map(\n",
        "        mapped_function,\n",
        "        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4_g5i9-MNTo",
        "colab_type": "text"
      },
      "source": [
        "### Caching\n",
        "The `tf.data.Dataset.cache` transformation can cache a dataset, either in memory or on local storate. This will save some operations (like file opening and data reading) from being executed during each epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzoujMu6MFGI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "benchmark(\n",
        "    ArtificialDataset()\n",
        "    .map( # Apply Time consuming operations before cache\n",
        "      mapped_function\n",
        "    ).cache(),\n",
        "    5\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nq_X1_kpMx1s",
        "colab_type": "text"
      },
      "source": [
        "When you cache a dataset, the transformations before the `cache` one (like the file opening and data reading) are executed only during first epoch. The next epochs will reuse the data cached by the `cache` transformation.\n",
        "\n",
        "If the user-defined function passed into the `map` transformation is expensive, apply the `cache` transformation after the `map` transformation as long as the resulting dataset can still fit into memory or local storage. If the user-defined function increases the space required to store the dataset beyond the cache capacity, either apply it after the `cache` transformation or consider pre-processing your data before your training job to reduce resource useage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVeBrRPjNuh8",
        "colab_type": "text"
      },
      "source": [
        "### Vectorizing mapping\n",
        "Invoking a user-defined function passed into the `map` transformation has overhead related to scheduling and executing the user-defined function. We recommend vetorizing the user-defined funciton (that is, have it operate over a batch of inputs at once) and apply the batch transformation before the `map` transformation.\n",
        "\n",
        "To illustrate this good practice, your artificial dataset is not suitable. The scheduling delay is around 10 microseconds, far less than the the tens of miliseconds used int `ArtificialDataset`, and thus its impacti is hard to see."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2l62hYhUMiyi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For this example, use the base `tf.data.Dataset.range` functoin\n",
        "# and simplify the training loop to its simplest form.\n",
        "fast_dataset = tf.data.Dataset.range(10000)\n",
        "\n",
        "def fast_benchmark(dataset, num_epochs=2):\n",
        "  start_time = time.perf_counter()\n",
        "  for _ in tf.data.Dataset.range(num_epochs):\n",
        "    for _ in dataset:\n",
        "      pass\n",
        "  tf.print('Execution time:', time.perf_counter() - start_time)\n",
        "\n",
        "def increment(x):\n",
        "  return x + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPmsfHDlOkgN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fast_benchmark(fast_dataset\n",
        "              .map(increment) # Apply function one item at a time\n",
        "              .batch(256) # Batch\n",
        "               )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6S7pDAVnOupc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Vectorized mapping\n",
        "fast_benchmark(\n",
        "    fast_dataset\n",
        "    .batch(256)\n",
        "    # Apply function on a batch of items\n",
        "    # The tf.Tensor.__add__ method already handled batches\n",
        "    .map(increment)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-WQ0_OcPRfQ",
        "colab_type": "text"
      },
      "source": [
        "### Reducing memory footprint\n",
        "A number of transformations, including `interleave`, `prefetch`, and `shuffle`, maintains an internal buffer of elements. If the user-defined function passed into the `map` transformation changes the size of elements, then the ordering of the map transformation and the transformations that buffer elements affects the memory usage. In general, we recommend choosing the order that results in lower memory footprint, unless different ordering is desirable for performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ny4-atkxPzcb",
        "colab_type": "text"
      },
      "source": [
        "### Caching partial computations\n",
        "It is recommended to cache the dataset after `map` transformation except if this transformation makes the data too big to fit in memory. A trade-off can be achived if your mapped function can be split in two parts: a time consuming one and a memory consuming part. In this case, you can chian your transformation like blow:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22C1xM5BO_y4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dataset.map(time_consuming_mapping).cahce().map(memory_consuming_mapping)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiMTOetaQMry",
        "colab_type": "text"
      },
      "source": [
        "This way, the time consuming part is only executed during the first epoch, and you avoid using too much cache space."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxLehPCNQUXs",
        "colab_type": "text"
      },
      "source": [
        "# Best practice summary\n",
        "- Use the `prefetch` transformation to overlap the work of a producer and consumer.\n",
        "- Parallelize the data reading transformation using the `interleave` transformation\n",
        "- Parallelize the `map` transformation by setting the `num_parallel_calls` argument.\n",
        "- Use the `cache` transformation to cache data in memory during the first epoch\n",
        "- Vectorize user-defined functions passed in to the `map` transformation\n",
        "- Reduce memory usage when applying the `interleave`, `prefetch`, and `shuffle` transformations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEwefFsfQztl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}