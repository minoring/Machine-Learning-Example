{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tf_data.ipynb",
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GKx_dtn9CYL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFLm-J4F-guv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import pathlib\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "np.set_printoptions(precision=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYsMtcUg-pW8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices([8, 3, 0, 8, 2, 1])\n",
        "dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wL2ke9GU_R7a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for elem in dataset:\n",
        "  print(elem.numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NANGgmly_Vxe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dataset object is Python iterable\n",
        "# Explicitly creating a Python iterator using `iter`\n",
        "# and comsuming its elements using `next`\n",
        "it = iter(dataset)\n",
        "print(next(it).numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXFZ4Iq1_k1v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(next(it).numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxYWyVd8_o-7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reduce transformation, which reduces all elements to produce a single result.\n",
        "print(dataset.reduce(0, lambda state, value: state + value).numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46b9JJzyATxg",
        "colab_type": "text"
      },
      "source": [
        "### Dataset structure\n",
        "A dataset contains elements that each have the same (nested) structure and the individual components of the structure can be of any type representable by `tf.TypeSpec`, including `Tensor`, `SparseTensor`, `RaggedTensor`, `TensorArray`, or `Dataset`\n",
        "\n",
        "The `Dataset.element_spec` property allows you to inspect the type of each element component. The property returns a nested structure of `tf.TypeSpec` object, matching the structure of the element, which may . be a single component, a tuple of components, or a nested tuple of components. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZP_GLsFM_5eU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset1 = tf.data.Dataset.from_tensor_slices(tf.random.uniform([4, 10]))\n",
        "dataset1.element_spec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9MKZiuoBzox",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset2 = tf.data.Dataset.from_tensor_slices(\n",
        "    (tf.random.uniform(shape=[4]),\n",
        "     tf.random.uniform(shape=[4, 100], maxval=100, dtype=tf.int32)))\n",
        "\n",
        "dataset2.element_spec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrQ1rHfPCZWb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset3 = tf.data.Dataset.zip((dataset1, dataset2))\n",
        "\n",
        "dataset3.element_spec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sU4lqc6JC672",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dataset containing a sparse tensor\n",
        "dataset4 = tf.data.Dataset.from_tensors(tf.SparseTensor(indices=[[0, 0], [1, 2]], values=[1, 2], dense_shape=[3, 4]))\n",
        "\n",
        "dataset4.element_spec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbdZUOPLDLIJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use value_type to see the type of value represented by the element spec\n",
        "dataset4.element_spec.value_type"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byacbx_XDt8i",
        "colab_type": "text"
      },
      "source": [
        "The `Dataset` transformations support datasets of any structure. When using the `Dataset.map()`, and `Dataset.filter()` transformations, which apply a function to each element, the element structure determines the arguments of the funciton:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ttx95iDsDY3o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset1 = tf.data.Dataset.from_tensor_slices(\n",
        "    tf.random.uniform([4, 10], minval=1, maxval=10, dtype=tf.int32))\n",
        "\n",
        "dataset1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CAnIhxMECRl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "next(iter(dataset1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XEcgMlCEFTX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for z in dataset1:\n",
        "  print(z.numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-eCyTfMEKyK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset2 = tf.data.Dataset.from_tensor_slices(\n",
        "    (tf.random.uniform([4]),\n",
        "     tf.random.uniform([4, 100], maxval=100, dtype=tf.int32)))\n",
        "\n",
        "dataset2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iw01czpKEWRS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset3 = tf.data.Dataset.zip((dataset1, dataset2))\n",
        "\n",
        "dataset3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7aGQRO4UEeow",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for a, (b, c) in dataset3:\n",
        "  print('shape: {a.shape}, {b.shape}, {c.shape}'.format(a=a, b=b, c=c))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kZMGtrNFbB_",
        "colab_type": "text"
      },
      "source": [
        "## Reading input data\n",
        "### Comsuming NumPy arrays\n",
        "See Loading NumPy arrays for more examples.\n",
        "If all of your input data fits in memory, the simplest way to create a `Dataset` from them is to convert them ti `tf.Tensor` objects and use `Dataset.from_tensor_slices()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtBuGkTYErqk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train, test = tf.keras.datasets.fashion_mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WadtNmedF123",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "images, labels = train\n",
        "images = images / 255.\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
        "dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jiOnBFAGkbq",
        "colab_type": "text"
      },
      "source": [
        "The above code snippet will embed the features and labels arrays in your TensorFlow graph as `tf.constant()` operations. This works well for small dataset, but wastes memory--because the contents of the array will be copied multiple times--and can run into the the 2GB limit for the `tf.GraphDef` protocol buffer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFkNBSYgHjdD",
        "colab_type": "text"
      },
      "source": [
        "### Consuming Python generators\n",
        "Another common data source that can easily be ingested as a `tf.data.Dataset` is the python generator.\n",
        "Caution: While this is a convinient apporach it has limited portability and scalibility. It mus run in the same python process that created the generator, and is sill subject to the Python GIL."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHXLHaIYGRNY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def count(stop):\n",
        "  i = 0\n",
        "  while i < stop:\n",
        "    yield i\n",
        "    i += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_dYJH0QIBUT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for n in count(5):\n",
        "  print(n)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NuiKVKOIMFX",
        "colab_type": "text"
      },
      "source": [
        "The `Dataset.from_generator` constructor converts the python generator to a fully functional `tf.data.Dataset`.\n",
        "\n",
        "The constructor takes a callable as input, not an iterator. This allows it to restart the generator when it reaches the end. It takes an optional `args` argument, which is passed as the callable's arguments.\n",
        "\n",
        "The `output_types` argument is required because `tf.data` builds a `tf.Graph` internally, and graph edges require a `tf.dtype`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAPrew_wIHBr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ds_counter = tf.data.Dataset.from_generator(count, args=[25], output_types=tf.int32, output_shapes=(),)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YA5-apW5JcMF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for count_batch in ds_counter.repeat().batch(10).take(10):\n",
        "  print(count_batch.numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGu-AsvIJi5t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gen_series():\n",
        "  i = 0\n",
        "  while True:\n",
        "    size = np.random.randint(0, 10)\n",
        "    yield i, np.random.normal(size=(size,))\n",
        "    i += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wo2fNk-FKNoa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i, series in gen_series():\n",
        "  print(i, ':', str(series))\n",
        "  if i > 5:\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5qfqAelKT2u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The first output is an `tf.int32` the second is a float32\n",
        "# The first item is a scalar, shape (), and the second is a vector of unknown length,\n",
        "# (None, )\n",
        "ds_series = tf.data.Dataset.from_generator(\n",
        "    gen_series,\n",
        "    output_types=(tf.int32, tf.float32),\n",
        "    output_shapes=((), (None, )))\n",
        "\n",
        "ds_series"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbcYZx_VK1Lg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now is can be used like a regular `tf.data.Dataset`.\n",
        "# Note that when batching a dataset with a variable shape,\n",
        "# you need to use `Dataset.padded_batch`.\n",
        "\n",
        "ds_series_batch = ds_series.shuffle(20).padded_batch(10, padded_shapes=([], [None]))\n",
        "\n",
        "ids, sequence_batch = next(iter(ds_series_batch))\n",
        "print(ids.numpy())\n",
        "print()\n",
        "print(sequence_batch.numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0jR7shiLNB-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "flowers = tf.keras.utils.get_file(\n",
        "    'flower_photos',\n",
        "    'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',\n",
        "    untar=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEhABXjULet7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the `image.ImageDataGenerator`\n",
        "img_gen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255, rotation_range=20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yXkonWPLo_D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "images, labels = next(img_gen.flow_from_directory(flowers))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Dcq2SrtLtXq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(images.dtype, images.shape)\n",
        "print(labels.dtype, labels.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3no3yLEPLxhD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ds = tf.data.Dataset.from_generator(\n",
        "    img_gen.flow_from_directory,\n",
        "    args=[flowers],\n",
        "    output_types=(tf.float32, tf.float32),\n",
        "    output_shapes=([32, 256, 256, 3], [32, 5]))\n",
        "\n",
        "ds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQDVARGeMnXT",
        "colab_type": "text"
      },
      "source": [
        "### Consuming TFRecord data\n",
        "\n",
        "TFRecord file format is a simple record-oriented binary format that many TensorFlow applications use for training data. The `tf.data.TFRecordDataset` class enables you to stream over the contents of one or more TFRecord files as part of an input pipeline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1L4fE3VsMNGs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creates a dataset that reads all of the examples from two files.\n",
        "fsns_test_file = tf.keras.utils.get_file(\"fsns.tfrec\", \"https://storage.googleapis.com/download.tensorflow.org/data/fsns-20160927/testdata/fsns-00000-of-00001\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8qpesnjNREB",
        "colab_type": "text"
      },
      "source": [
        "The `filenames` argument to the `TFRecordDataset` initializer can be either be string, or a `tf.Tensor` of strings. Therefore if you have two sets of files for training and validation purposes, you can create a factory method that produces the dataset, taking filenames as an input argument:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXGFf45bNGM8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = tf.data.TFRecordDataset(filenames = [fsns_test_file])\n",
        "dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haL0ZWRGOCAf",
        "colab_type": "text"
      },
      "source": [
        "Many TensorFlow projects use serialized `tf.train.Example` records in their TFRecord files. These need to be decoded before they can be inspected:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qs7ahWBmN6VG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "raw_example = next(iter(dataset))\n",
        "parsed = tf.train.Example.FromString(raw_example.numpy())\n",
        "\n",
        "parsed.features.feature['image/text']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-si2qfGOW-S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tf.data.TextLineDataset extract lines from one or more text files.\n",
        "# TextLineDataset will produce one string-valued element per line of those files.\n",
        "\n",
        "directory_url = 'https://storage.googleapis.com/download.tensorflow.org/data/illiad/'\n",
        "file_names = ['cowper.txt', 'derby.txt', 'butler.txt']\n",
        "\n",
        "file_paths = [\n",
        "    tf.keras.utils.get_file(file_name, directory_url + file_name)\n",
        "    for file_name in file_names]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYS9ZqmFOs_T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = tf.data.TextLineDataset(file_paths)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVApfloUPPLq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Few lines of the first file:\n",
        "for line in dataset.take(5):\n",
        "  print(line.numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMZ6qvWlPSrn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To alternate lines between files use `Dataset.interleave`.\n",
        "# This makes it easier to shuffle files together.\n",
        "file_ds = tf.data.Dataset.from_tensor_slices(file_paths)\n",
        "lines_ds = file_ds.interleave(tf.data.TextLineDataset, cycle_length=3)\n",
        "\n",
        "for i, line in enumerate(lines_ds.take(9)):\n",
        "  if i % 3 == 0:\n",
        "    print()\n",
        "  print(line.numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHAXTGDOQYhC",
        "colab_type": "text"
      },
      "source": [
        "By default, a `TextLineDataset` yield every line of each file, which may not be desirable, for example, if the file starts with a header line, or contains comments. These lines can be removed using the `Dataset.skip()` or `Dataset.filter()` transformation. Here we skip the first line, then filter to fine only survivors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OEFL4E8QSw2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "titanic_file = tf.keras.utils.get_file(\"train.csv\", \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\")\n",
        "titanic_lines = tf.data.TextLineDataset(titanic_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ktzaPBgQvv_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for line in titanic_lines.take(10):\n",
        "  print(line.numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZpOHLB8Q0Sl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def survived(line):\n",
        "  return tf.not_equal(tf.strings.substr(line, 0, 1), '0')\n",
        "\n",
        "survivors = titanic_lines.skip(1).filter(survived) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d27CmLz3RM0L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for line in survivors.take(10):\n",
        "  print(line.numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGGEqFi_RXEV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiiHLhMrRami",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}