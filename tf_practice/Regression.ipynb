{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Regression.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fjuqWp3-bqQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Regression: Predict fuel efficiency\n",
        "# predict the output of continuous value like price of a probability\n",
        "# Use Auto MPG dataset and builds model to predict the fuel efficiency."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nSawNdt_fQu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use seaborn for pairplot\n",
        "!pip install -q seaborn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IwAK9OU_iRC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import pathlib\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "try:\n",
        "    %tensorflow_version 2.x\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PL6WsW3B_5xR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Auto MPG dataset\n",
        "# The dataset is available from UCI Machine Learning Repository\n",
        "\n",
        "# Get data\n",
        "dataset_path = keras.utils.get_file('auto-mpg.data', \n",
        "        'http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data')\n",
        "dataset_path"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpYpuR38AY8c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "column_names = ['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight',\n",
        "                'Acceleration', 'Model Year', 'Origin']\n",
        "raw_dataset = pd.read_csv(dataset_path, names=column_names, na_values='?', \n",
        "                          comment='\\t', sep=' ', skipinitialspace=True)\n",
        "dataset = raw_dataset.copy()\n",
        "dataset.tail()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEJd9zc6Bjyb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "column_names = ['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight',\n",
        "                'Acceleration', 'Model Year', 'Origin']\n",
        "raw_dataset = pd.read_csv(dataset_path, names=column_names, na_values='?',\n",
        "                          skipinitialspace=True, comment='\\t', sep=' ')\n",
        "dataset = raw_dataset.copy()\n",
        "dataset.tail()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xaCSl4aMAzXp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Clean data\n",
        "# The dataset contains a few unknown values\n",
        "dataset.isna()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoHIsYgRDoTo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset.isna().sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ur9W0gdUDvmJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# drop those rows\n",
        "dataset = dataset.dropna()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLGOjCOPD1mo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# column 'Origin' is categorical.\n",
        "# So conver that to a one-hot.\n",
        "\n",
        "origin = dataset.pop('Origin')\n",
        "\n",
        "dataset['USA'] = (origin == 1) * 1.0\n",
        "dataset['Europe'] = (origin == 2) * 1.0\n",
        "dataset['Japan'] = (origin == 3) * 1.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wGmZ4SwEiFb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset.index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXfMNTntEYG2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split the data into train and test\n",
        "train_dataset = dataset.sample(frac=0.8, random_state=0)\n",
        "test_dataset = dataset.drop(train_dataset.index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3t_Gljd_Er6m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Inspect the data\n",
        "# Joint distribution of a few pairs of column from the training dataset.\n",
        "sns.pairplot(train_dataset[['MPG', 'Cylinders', 'Displacement', 'Weight']], \n",
        "             diag_kind='kde')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2UbpG6CFg13",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Look at the overall statistics:\n",
        "train_stats = train_dataset.describe()\n",
        "train_stats.pop('MPG')\n",
        "train_stats = train_stats.transpose()\n",
        "train_stats"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LQpO8rxGIMB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split features from labels\n",
        "train_labels = train_dataset.pop('MPG')\n",
        "test_labels = test_dataset.pop('MPG')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrV_11sHGY2t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Normalize the data\n",
        "# stats show hat how different the ranges of each feature are.\n",
        "# It is good practice to normalize features that use different scales and ranges\n",
        "# Although the model might converge without feature normalization, it makes training\n",
        "# more difficult, and it makes the resulting model dependent on the choice of units used in the input\n",
        "\n",
        "def norm(x):\n",
        "    return (x - train_stats['mean']) / train_stats['std']\n",
        "normed_train_data = norm(train_dataset)\n",
        "normed_test_data = norm(test_dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FskIQTpIUgr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Note:\n",
        "# The statistics used to normalize the inputs here (mean and standard devication)\n",
        "# need to be applied to any other data that is fed to the mode, along with the one-hot\n",
        "# encoding that we did eariler. That includes the test set as well as live data\n",
        "# when the model is used in production"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYRrGoq2J3DX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset.keys()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLQMKAA5I0wr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "    model = keras.Sequential([\n",
        "        layers.Dense(64, activation='relu', input_shape=[len(train_dataset.keys())]),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dense(1)\n",
        "    ])\n",
        "    \n",
        "    optimizer = tf.keras.optimizers.RMSprop(0.001)\n",
        "    \n",
        "    model.compile(loss='mse',\n",
        "                  optimizer=optimizer,\n",
        "                  metrics=['mae', 'mse'])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xL2CFL0QJboE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrH_g89LJdSn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Try out the model with 10 examples for the training data\n",
        "# and call model.predict on it.\n",
        "\n",
        "example_batch = normed_train_data[:10]\n",
        "example_result = model.predict(example_batch)\n",
        "example_result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02hpYeDPKNpo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train the model\n",
        "# Train the model for 1000 epochs, and record the training and validation accuracy\n",
        "# in the history object\n",
        "\n",
        "# Display training progress by printing a single dot for each completed epoch\n",
        "class PrintDot(keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs):\n",
        "        if epoch % 100 == 0:\n",
        "            print('')\n",
        "        print('.', end='')\n",
        "        \n",
        "EPOCHS = 1000\n",
        "\n",
        "history = model.fit(normed_train_data, train_labels, epochs=EPOCHS, \n",
        "                    validation_split=0.2, verbose=0, callbacks=[PrintDot()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xljGphcaK6VC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hist = pd.DataFrame(history.history)\n",
        "hist['epoch'] = history.epoch\n",
        "hist.tail()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1u1KejjaLKmF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_history(history):\n",
        "    hist = pd.DataFrame(history.history)\n",
        "    hist['epoch'] = history.epoch\n",
        "    \n",
        "    plt.figure()\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Mean Abs Error [MPG]')\n",
        "    plt.plot(hist['epoch'], hist['mae'],\n",
        "             label='Train Error')\n",
        "    plt.plot(hist['epoch'], hist['val_mae'],\n",
        "             label='Val Error')\n",
        "    plt.ylim([0, 5])\n",
        "    plt.legend()\n",
        "    \n",
        "    plt.figure()\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Mean Square Error[$MPG^2$]')\n",
        "    plt.plot(hist['epoch'], hist['mse'],\n",
        "            label='Train Error')\n",
        "    plt.plot(hist['epoch'], hist['val_mse'],\n",
        "             label='Val error')\n",
        "    plt.ylim([0, 20])\n",
        "    plt.legend()\n",
        "    \n",
        "    plt.show()\n",
        "    \n",
        "plot_history(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtxxsgFnMhRw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The graph shows degradation in the validation error after about\n",
        "# 100 epochs. Let's update model.fit call to automatically stop training when the validation\n",
        "# score doesn't improve.\n",
        "# Use an EarlyStopping callback that tests a training condition for every epoch.\n",
        "# If a set amount of epochs elapses without showing improvment,\n",
        "# ,then automatically stop the training\n",
        "\n",
        "model = build_model()\n",
        "\n",
        "# The patience parameter is the amount of epochs to check for improvment\n",
        "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
        "\n",
        "history = model.fit(normed_train_data, train_labels, epochs=EPOCHS,\n",
        "                    validation_split=0.2, verbose=0, callbacks=[early_stop, PrintDot()])\n",
        "plot_history(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGmVh7-4N5ok",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's see how well the model generalizes by using the test set.\n",
        "loss, mae, mse = model.evaluate(normed_test_data, test_labels, verbose=0)\n",
        "\n",
        "print('Testing set Mean Error: {:5.2f} MPG'.format(mae))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yL17VwfPOnjr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predict MPG values using data in the testing set:\n",
        "\n",
        "test_predictions = model.predict(normed_test_data).flatten()\n",
        "\n",
        "plt.scatter(test_labels, test_predictions)\n",
        "plt.xlabel('True Values [MPG]')\n",
        "plt.ylabel('Predictions [MPG]')\n",
        "# plt.axis('equal')\n",
        "plt.axis('square')\n",
        "plt.xlim([0, plt.xlim()[1]])\n",
        "plt.ylim([0, plt.ylim()[1]])\n",
        "_ = plt.plot([-100, 100], [-100, 100])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQEZtZPqPOr9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "error = test_predictions - test_labels\n",
        "plt.hist(error, bins = 25)\n",
        "plt.xlabel(\"Prediction Error [MPG]\")\n",
        "_ = plt.ylabel(\"Count\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kulzcobsQQGe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# It's not quite gaussian, but we might expect that because the number of samples is very small.\n",
        "# Reducible error and irreducible error\n",
        "# When numeric input data features have values with differnet ranges,\n",
        "# each feature should be scaled independently to the same range"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}