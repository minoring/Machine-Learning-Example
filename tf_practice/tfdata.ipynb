{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tfdata.ipynb",
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_YJOWFTv6Qj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import pathlib\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "np.set_printoptions(precision=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBgw-ciQwu_N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices([8, 3, 0, 8, 2, 1])\n",
        "dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEc6L70fxGcR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for elem in dataset:\n",
        "  print(elem.numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w39---6ExKU1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "it = iter(dataset)\n",
        "print(next(it).numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxO9etpJxRWu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset.reduce(0, lambda state, value: state + value).numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqoKwlsAy4qt",
        "colab_type": "text"
      },
      "source": [
        "#Dataset structure\n",
        "A dataset contains elements that each have the same (nested) structure and the individual components of the structure can be of any type representable by `tf.TypeSpec`, including `Tensor`, `SparseTensor`, `RaggedTensor`, `TensorArray`, or `Dataset`.\n",
        "\n",
        "The `Dataset.element_spec` property allows you to inspect the type of each element component. The property returns a nested structure of `tf.TypeSpec` object, matching the structure of the element, which may be a single component, a tuple of components, or a nested tuple of components."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAjHC0SgxafE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset1 = tf.data.Dataset.from_tensor_slices(tf.random.uniform([4, 10]))\n",
        "dataset1.element_spec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xcbo3LRRzd4q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset2 = tf.data.Dataset.from_tensor_slices(\n",
        "    (tf.random.uniform([4]),\n",
        "     tf.random.uniform([4, 100], maxval=100, dtype=tf.int32)))\n",
        "dataset2.element_spec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IUrDAvuzzbb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset3 = tf.data.Dataset.zip((dataset1, dataset2))\n",
        "dataset3.element_spec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ua33is8yz5ae",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dataset containing a sparse tensor.\n",
        "dataset4 = tf.data.Dataset.from_tensors(tf.SparseTensor(indices=[[0, 0], [1, 2]], values=[1, 2], dense_shape=[3, 4]))\n",
        "dataset4.element_spec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVaSViff0Oqu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use value_type to see the type of value represented by the element spec\n",
        "dataset4.element_spec.value_type\n",
        "# element_spec 은 tf.TypeSpec 객체를 반환하니까, 값의 타입은 거기에 value_type을 붙여준다."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnRCSfyI0gmE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset1 = tf.data.Dataset.from_tensor_slices(\n",
        "    tf.random.uniform([4, 10], minval=1, maxval=10, dtype=tf.int32))\n",
        "\n",
        "dataset1 # elem의 shape을 준다."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVKcqD921Fs3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for z in dataset1:\n",
        "  print(z.numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sm6Ew5Bm1Q0D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset2 = tf.data.Dataset.from_tensor_slices(\n",
        "    (tf.random.uniform([4]),\n",
        "     tf.random.uniform([4, 100], maxval=100, dtype=tf.int32)))\n",
        "dataset2\n",
        "# Dataset의 element의 type(structure)은 똑같고\n",
        "# element 원소의 의 type은 똑같다."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_aKtPAK1bCZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset3 = tf.data.Dataset.zip((dataset1, dataset2))\n",
        "dataset3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Pk4msnt1n7h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for a, (b, c) in dataset3:\n",
        "  print('shapes: {a.shape}, {b.shape}, {c.shape}'.format(a=a, b=b, c=c))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhzLW9GV18nT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train, test = tf.keras.datasets.fashion_mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xc4HhO6v3mmb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "images, labels = train\n",
        "images = images / 255.0\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
        "dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7c-Pz5634YC",
        "colab_type": "text"
      },
      "source": [
        "## Note\n",
        "The above code snippet will embed the features and laels array in your TensorFlow graph as tf.constant() operations. This works well for a small dataset, but wastes memory -- because the contents of the array will be copied multiple times -- and can run into the 2GB limit for the tf.GraphDef protocol buffer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zK-c04TV4Us6",
        "colab_type": "text"
      },
      "source": [
        "### Consuming Python generators.\n",
        "Another common data source that can easily be ingested as a `tf.data.Dataset` is the Python generator.\n",
        "\n",
        "Caution: While this is a convinient approach it has limited portability and scalibility. It must run in the same python process that created the generator, and is still subject to the Python GIL."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2JHjdOY3trn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def count(stop):\n",
        "  i = 0\n",
        "  while i < stop:\n",
        "    yield i\n",
        "    i += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bb4ygiZh4y34",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for n in count(5): # Generator functions behaves like an iterator.\n",
        "  print(n)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ac-Oo13U49mD",
        "colab_type": "text"
      },
      "source": [
        "The `Dataset.from_generator` constructor converts the python generator to a fully functional `tf.data.Dataset`.\n",
        "\n",
        "The constructor takes a callable as input, not an iterator. This allows it to restart the generator when it reaches the end. It takes an optional `args` argument, which is passed as the callable's arguments.\n",
        "\n",
        "The `output_types` argument is required because `tf.data` builds a `tf.Graph` internally, and graph edges require a `tf.dtype`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weD8_vy843Ha",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ds_counter = tf.data.Dataset.from_generator(count, args=[25], output_types=tf.int32, output_shapes=())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KS3t97A55nK6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for count_batch in ds_counter.repeat().batch(10).take(10):\n",
        "  print(count_batch.numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tov1pYZ07QI9",
        "colab_type": "text"
      },
      "source": [
        "The `output_shapes` argument is not required but is highly recommended as many tensorflow operations do not support tensors with unknown rank. If the length of a particular axis is unknown or variable, set it as `None` in the `output_shapes`.\n",
        "\n",
        "\n",
        "It's also important to note that the `output_shapes` and `output_types` follow the same nesting rules as other dataset methods.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hOOSL-T5xKe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Here is an example generator that demonstrates both aspects,\n",
        "# it returns tuple of arrays, where the second array is a vector with unknown length.\n",
        "def gen_series():\n",
        "  i = 0\n",
        "  while True:\n",
        "    size = np.random.randint(0, 10)\n",
        "    yield i, np.random.normal(size=(size, ))\n",
        "    i += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhZaQa698Oea",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i, series in gen_series():\n",
        "  print(i, ':', str(series))\n",
        "  if i > 5:\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlJItBCj8X6i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ds_series = tf.data.Dataset.from_generator(gen_series, \n",
        "                                 output_types=(tf.int32, tf.float32), \n",
        "                                 output_shapes=((), (None, )))\n",
        "ds_series"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wc3W0gZ_8ycu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now it can be used like a regular `tf.data.Dataset`.\n",
        "# Note that when batching a dataset with a variable shape,\n",
        "# you need to use `Dataset.padded_batch`.\n",
        "\n",
        "ds_series_batch = ds_series.shuffle(20).padded_batch(10, padded_shapes=((), (None, )))\n",
        "\n",
        "ids, sequence_batch = next(iter(ds_series_batch))\n",
        "print(ids.numpy())\n",
        "print()\n",
        "print(sequence_batch.numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3HlHc7V-G5L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For a more realistic example, try wrapping `preprocessing.image.ImageDataGenerator`\n",
        "# as a `tf.data.Dataset`.\n",
        "\n",
        "# First download the data.\n",
        "flowers = tf.keras.utils.get_file(\n",
        "  'flower_photos',\n",
        "  'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',\n",
        "  untar=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRGhHcgM-Yl2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "flowers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dRr_6jq-a-S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the `image.ImageDataGenerator`\n",
        "img_gen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255, rotation_range=20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-WFolWA-5Z1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "images, labels = next(img_gen.flow_from_directory(flowers))\n",
        "# args are files.\n",
        "# image_gen.flow_from_directory is generator because it returns iterator."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4bG7W9g_FGu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(images.dtype, images.shape)\n",
        "print(labels.dtype, labels.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14Vi7sl__LiN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ds = tf.data.Dataset.from_generator(img_gen.flow_from_directory, \n",
        "                                    args=[flowers], \n",
        "                                    output_types=(tf.float32, tf.float32), \n",
        "                                    output_shapes=((32, 256, 256, 3), (32, 5)))\n",
        "ds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeVpIfE7_1wr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creates a dataset that reads all of examples from two files.\n",
        "fsns_test_file = tf.keras.utils.get_file(\"fsns.tfrec\", \"https://storage.googleapis.com/download.tensorflow.org/data/fsns-20160927/testdata/fsns-00000-of-00001\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M99P2CDB371r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = tf.data.TFRecordDataset(filenames=[fsns_test_file])\n",
        "dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EU7oGfG4hTZ",
        "colab_type": "text"
      },
      "source": [
        "Many TensorFlow projects use serialized `tf.train.Example` records in their TFRecord files. These need to be decoded before they can be inspected."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8P0B8b3D4VbM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "raw_example = next(iter(dataset))\n",
        "parsed = tf.train.Example.FromString(raw_example.numpy())\n",
        "\n",
        "parsed.features.feature['image/text']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fra9jDb5GNQ",
        "colab_type": "text"
      },
      "source": [
        "The `tf.data.TextLineDataset` provides an easy way to extract lines from one or more text files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPb5qAx64y6r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "directory_url = 'https://storage.googleapis.com/download.tensorflow.org/data/illiad/'\n",
        "file_names = ['cowper.txt', 'derby.txt', 'butler.txt']\n",
        "\n",
        "file_paths = [tf.keras.utils.get_file(file_name, directory_url + file_name)\n",
        "  for file_name in file_names\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBbFIsHC5l2G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = tf.data.TextLineDataset(file_paths) # 파일을 하나로 뭉친것처럼."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voCZSQ8t5rNP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for line in dataset.take(5):\n",
        "  print(line.numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFLuUzwF50Qq",
        "colab_type": "text"
      },
      "source": [
        "To alternate lines between files use `Dataset.interleave`. This makes it easier to shuffle files together."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqB5zp525u-d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files_ds = tf.data.Dataset.from_tensor_slices(file_paths) # 파일 여러개, 배열.\n",
        "lines_ds = files_ds.interleave(tf.data.TextLineDataset, cycle_length=3) # interleave.\n",
        "\n",
        "for i, line in enumerate(lines_ds.take(9)):\n",
        "  if i % 3 == 0:\n",
        "    print()\n",
        "  print(line.numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7byeLo156KRE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "next(iter(files_ds))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvT1_uyT7kHW",
        "colab_type": "text"
      },
      "source": [
        "By default, a `TextLineDataset` yields every line of each file, which may not be desirable, for example, if the file starts with a header line, or contains comments. These lines can be removed using the `Dataset.skip()` or `Dataset.filter()` transformations. Here we skip the first line, then filter to fine only survivors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxVkF1vc7E-3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "titanic_file = tf.keras.utils.get_file(\"train.csv\", \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\")\n",
        "titanic_lines = tf.data.TextLineDataset(titanic_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k56RBSyj77Nh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for line in titanic_lines.take(10):\n",
        "  print(line.numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgG_j6zs7-t0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def survived(line):\n",
        "  return tf.not_equal(tf.strings.substr(line, 0, 1), '0')\n",
        "\n",
        "survivors = titanic_lines.skip(1).filter(survived)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxej6fMt8Wet",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for line in survivors.take(10):\n",
        "  print(line.numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ol_cZz5s8aNs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "titanic_file = tf.keras.utils.get_file(\"train.csv\", \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFb3cbOeA1J5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(titanic_file, index_col=None)\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4u-Zy0kBEpg",
        "colab_type": "text"
      },
      "source": [
        "If your data fits in memory the same `Dataset.from_tensor_slices` method works on dictionaries, allowing this data to be easily imported."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7v9ixO2A5Mi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "titanic_slices = tf.data.Dataset.from_tensor_slices(dict(df))\n",
        "\n",
        "for feature_batch in titanic_slices.take(1):\n",
        "  for key, value in feature_batch.items():\n",
        "    print('  {!r:20s}: {}'.format(key, value))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imW8SWg3BaNp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "titanic_batches = tf.data.experimental.make_csv_dataset(titanic_file, batch_size=4, label_name='survived')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzAu8Lm2BrY1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for feature_batch, label_batch in titanic_batches.take(1):\n",
        "  print('Survived: {}'.format(label_batch))\n",
        "  print('features:')\n",
        "  for key, value in feature_batch.items():\n",
        "    print('   {!r:20s}: {}'.format(key, value))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-_mUfTyB7Lj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# You can use `select_columns` argument if you only need a subset of columns.\n",
        "titanic_batches = tf.data.experimental.make_csv_dataset(\n",
        "    titanic_file, batch_size=4,\n",
        "    label_name='survived', select_columns=['class', 'fare', 'survived']\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fm9WsDuhCO6a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for feature_batch, label_batch in titanic_batches.take(1):\n",
        "  print(\"'survived': {}\".format(label_batch))\n",
        "  for key, value in feature_batch.items():\n",
        "    print(\"  {!r:20s}: {}\".format(key, value))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DPnwBO8CVvn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Consuming sets of files.\n",
        "flowers_root = tf.keras.utils.get_file(\n",
        "    'flower_photos',\n",
        "     'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',\n",
        "     untar=True\n",
        ")\n",
        "flowers_root= pathlib.Path(flowers_root) # pathlib.Path로 감싸줘서 glob쓴다.\n",
        "# Gives semantic file path for different OS"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0m8Lzf8tUGC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "flowers_root"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wrCeJBKvu_2",
        "colab_type": "text"
      },
      "source": [
        "1. utils.get_file로 파일 다운로드, 디렉토리 경로 잡고,\n",
        "2. pathlib.Path 로 general 한 file path 만들고, \n",
        "3. Dataset.list_files로 파일 리스트로 잡고\n",
        "4. map으로 전처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afRuL22QtZmA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The root directory contains a directory for each class:\n",
        "for item in flowers_root.glob('*'):\n",
        "  print(item)\n",
        "  print(item.name) # Glob에 name하면 파일(디렉터리) 이름 나온다."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQRr01zaujLo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The files in each class directory are example.\n",
        "list_ds = tf.data.Dataset.list_files(str(flowers_root/'*/*' ))\n",
        "\n",
        "for f in list_ds.take(5):\n",
        "  print(f.numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VceDQwOtqYt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We can read the data using the `tf.io.read_file` function and extract\n",
        "# the label from the path, returning (image, label) pairs.\n",
        "def process_path(file_path): # Dataset이 되는구나.\n",
        "  label = tf.strings.split(file_path, '/')[-2]\n",
        "  return tf.io.read_file(file_path), label\n",
        "\n",
        "labeled_ds = list_ds.map(process_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOvfmMG8vKDe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for image_raw, label_text in labeled_ds.take(1):\n",
        "  print(repr(image_raw.numpy()[:100]))\n",
        "  print()\n",
        "  print(label_text.numpy())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78dybyKEwRNa",
        "colab_type": "text"
      },
      "source": [
        "Batching dataset elements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X24yskMLvSBu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Batching does with same constraints as tf.stack()\n",
        "# all elements must have a tensor of the exact same shape.\n",
        "inc_dataset = tf.data.Dataset.range(100)\n",
        "dec_dataset = tf.data.Dataset.range(0, -100, -1)\n",
        "dataset = tf.data.Dataset.zip((inc_dataset, dec_dataset))\n",
        "print(dataset.element_spec)\n",
        "batched_dataset = dataset.batch(4)\n",
        "print(batched_dataset.element_spec) # Batch하면 element_spec 다르다. (당연하지 한번에 여러개가 나오는데.)\n",
        "\n",
        "for batch in batched_dataset.take(4):\n",
        "  print([arr.numpy() for arr in batch])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYa_FrrKxbzA",
        "colab_type": "text"
      },
      "source": [
        "While `tf.data` tries to propagate shape information, the default settings of `Dataset.batch` result in an unknown batch size because the last batch may not be full."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1pA3jyfxG6Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Note the `None`s in the shape\n",
        "batched_dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFxBcdewx5_z",
        "colab_type": "text"
      },
      "source": [
        "Use the `drop_remainder` argument to ignore the last batch, and get full shape propagation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4387-3BIxpKt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batched_dataset = dataset.batch(7, drop_remainder=True)\n",
        "batched_dataset # 여기서  shape는 element의 shape이다."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOXi8H3Jygzd",
        "colab_type": "text"
      },
      "source": [
        "### Batching tensors with padding\n",
        "The above works for tensors that all have the same size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZllYJl_LyDQo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = tf.data.Dataset.range(100)\n",
        "dataset = dataset.map(lambda x: tf.fill([tf.cast(x, tf.int32)], x))\n",
        "dataset = dataset.padded_batch(4, padded_shapes=(None,))\n",
        "\n",
        "for batch in dataset.take(2):\n",
        "  print(batch.numpy())\n",
        "  print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huMrVAfszY_H",
        "colab_type": "text"
      },
      "source": [
        "## Training workflows\n",
        "\n",
        "Iterate over a dataset in multiple epochs using the `Dataset.repeat()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vnbLtOczDca",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "titanic_file = tf.keras.utils.get_file('train.csv', \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\")\n",
        "titanic_lines = tf.data.TextLineDataset(titanic_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DG4gQ_GFzs3Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_batch_size(ds):\n",
        "  batch_sizes = [batch.shape[0] for batch in ds]\n",
        "  plt.bar(range(len(batch_sizes)), batch_sizes)\n",
        "  plt.xlabel('Batch number')\n",
        "  plt.ylabel('Batch size')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDhmTG340I7f",
        "colab_type": "text"
      },
      "source": [
        "The `Dataset.repeat` transformation concatenates its arguments without signaling the end of one epoch and the beginning of the next epoch. Because of this a `Dataset.batch` applied after `Dataset.repeat` will yield batches that straddle epoch boundaries:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4A6h4Xnz8rZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Applying the `Dataset.repeat()` transformation with no arguments will repeat the\n",
        "# input indefinitely.\n",
        "titanic_batches = titanic_lines.repeat(3).batch(128)\n",
        "plot_batch_size(titanic_batches)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zawFy5ky03zN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# If you need clear epoch separation,\n",
        "# put `Dataset.batch` before repeat:\n",
        "titanic_batches = titanic_lines.batch(128).repeat(3)\n",
        "plot_batch_size(titanic_batches)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3i9XPaNc2BLt",
        "colab_type": "text"
      },
      "source": [
        "If you would like to perform a custom computation (e.g. to collect statistics) at the end of each epoch then it's simplest to restart the dataset iteration on ecah epoch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ETaPknt1niw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 3\n",
        "dataset = titanic_lines.batch(128)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  for batch in dataset:\n",
        "    print(batch.shape)\n",
        "    print(batch[0]) # 128개의 line tensor\n",
        "  print('End of epoch: ', epoch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ow2Y5hnd3YyV",
        "colab_type": "text"
      },
      "source": [
        "### Randomly shuffling input data\n",
        "The `Dataset.shuffle()` transformation maintains a fixed_size buffer and choose the next element uniformly at random from that buffer.\n",
        "\n",
        "While large buffer_size shuffle more thorougly, they can take a lot of memory, and significant time to fill. Consider using Dataset.interleave across files if this becomes a problem.\n",
        "\n",
        "Dataset.interleave는 여러 파일이라서 shuffle한것처럼 됨."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ht67MUxk2b1Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Add an index to the dataset so you can see the effect:\n",
        "lines = tf.data.TextLineDataset(titanic_file)\n",
        "counter = tf.data.experimental.Counter()\n",
        "\n",
        "dataset = tf.data.Dataset.zip((counter, lines))\n",
        "dataset = dataset.shuffle(buffer_size=100)\n",
        "dataset = dataset.batch(20, drop_remainder=True)\n",
        "dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gs0_jVNw4vcq",
        "colab_type": "text"
      },
      "source": [
        "Since the `buffer_size` is 100, and the batch size is 20, the first batch contains no elements with an index over 120."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCzavAzm4XrW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n, line_batch = next(iter(dataset))\n",
        "print(n.numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXE5jeyu5OQ3",
        "colab_type": "text"
      },
      "source": [
        "As with `Dataset.batch` the order relative to `Dataset.repeat` matters.\n",
        "\n",
        "`Dataset.shuffle` doesn't signal the end of an epoch until the shuffle buffer is empty. So a shuffle placed before a repeat will show every element of one epoch before moving to the next."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVeT8eh65F5j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = tf.data.Dataset.zip((counter, lines))\n",
        "shuffled = dataset.shuffle(buffer_size=100).batch(10).repeat(2)\n",
        "\n",
        "print('Here are the item IDs near the epoch boundary:\\n')\n",
        "for n, line_batch in shuffled.skip(60).take(5):\n",
        "  print(n.numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eM3qaW-K7qbW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "shuffle_repeat = [n.numpy().mean() for n, line_batch in shuffled]\n",
        "plt.plot(shuffle_repeat, label='shuffle().repeat()')\n",
        "plt.ylabel('Mean item ID')\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdjqivYn1iyo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# But a repeat before a shuffle mixes the epoch boundaries together:\n",
        "dataset = tf.data.Dataset.zip((counter, lines))\n",
        "shuffled = dataset.repeat(2).shuffle(buffer_size=100).batch(10)\n",
        "\n",
        "print('Here are the item IDs near the epoch boundary:\\n')\n",
        "for n, line_batch in shuffled.skip(55).take(15):\n",
        "  print(n.numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41fqREvR17Gt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "repeat_shuffle = [n.numpy().mean() for n, line_batch in shuffled]\n",
        "\n",
        "plt.plot(shuffle_repeat, label='shuffle().repeat()')\n",
        "plt.plot(repeat_shuffle, label='repeat().shuffle()')\n",
        "plt.ylabel('Mean item ID')\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywIf_qOF2gYp",
        "colab_type": "text"
      },
      "source": [
        "## Decoding image data and resizing it\n",
        "When training a neural network on real-world image data, it is often necessary to convert images of different sizes to a common size, so that they may be bached into a fized size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XB-oBqos2IuN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "list_ds = tf.data.Dataset.list_files(str(flowers_root/'*/*'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIL4a_uZ2zCF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for f in list_ds.take(5):\n",
        "  print(f.numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRYH9SeZ3E5n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read an image from a file, decodes it into a dense tensor,\n",
        "# and resizes it to a fixed shape.\n",
        "def parse_image(filename):\n",
        "  parts = tf.strings.split(filename, '/')\n",
        "  label = parts[-2]\n",
        "\n",
        "  image = tf.io.read_file(filename)\n",
        "  image = tf.image.decode_jpeg(image)\n",
        "  image = tf.image.convert_image_dtype(image, tf.float32)\n",
        "  image = tf.image.resize(image, [128, 128])\n",
        "  return image, label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPKQbgjM3gHm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test that it works.\n",
        "file_path = next(iter(list_ds))\n",
        "image, label = parse_image(file_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dv5pyuSL3oRl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show(image, label):\n",
        "  plt.figure()\n",
        "  plt.imshow(image)\n",
        "  plt.title(label.numpy().decode('utf-8'))\n",
        "  plt.axis('off')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4wh7Mvj3tts",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "show(image, label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ah6iz7wE3ur7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Map it over the dataset\n",
        "image_ds = list_ds.map(parse_image)\n",
        "\n",
        "for image, label in image_ds.take(3):\n",
        "  show(image, label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6ysV_6c4dUU",
        "colab_type": "text"
      },
      "source": [
        "### Applying arbitrary Python logic\n",
        "\n",
        "For performance reasons, we encourage you to use TensorFlow operations for preprocessing your data whenevert possible. However, it is sometimes usefult to call external Python libraries when parsing your input data. You can use the `tf.py_function()` operation in a `Dataset.map()` transoformation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UqqOFE932F4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To demonstrate `tf.py_function`,\n",
        "# try using the `scipy.ndimage.rotate` function instead:\n",
        "import scipy.ndimage as ndimage\n",
        "\n",
        "def random_rotate_image(image):\n",
        "  image = ndimage.rotate(image, np.random.uniform(-30, 30), reshape=False)\n",
        "  return image "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_otXgMf4-T1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image, label = next(iter(image_ds))\n",
        "image = random_rotate_image(image)\n",
        "show(image, label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeP0dRCu5apO",
        "colab_type": "text"
      },
      "source": [
        "To use this function with `Dataset.map` the same caveats apply as with `Dataset.from_generator`, you need to describe toe return shapes and types when you apply the function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PA4x6cB45FUt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tf_random_rotate_image(image, label):\n",
        "  im_shape = image.shape\n",
        "  [image, ] = tf.py_function(random_rotate_image, [image], [tf.float32])\n",
        "  image.set_shape(im_shape)\n",
        "  return image, label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5_bG5Vd5_8P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rot_ds = image_ds.map(tf_random_rotate_image)\n",
        "\n",
        "for image, label in rot_ds.take(2):\n",
        "  show(image, label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmhGmGVp67mP",
        "colab_type": "text"
      },
      "source": [
        "### Parsing tf.Example protocol buffer message\n",
        "\n",
        "Many input  pipelines extract `tf.train.Example` protocol buffer messages from a TFRecord format. Each `tf.train.Example` record contains one or more 'features', and the input pipeline typically converts these features into tensors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6ULkAXA6GzC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fsns_test_file = tf.keras.utils.get_file(\"fsns.tfrec\", \"https://storage.googleapis.com/download.tensorflow.org/data/fsns-20160927/testdata/fsns-00000-of-00001\")\n",
        "dataset = tf.data.TFRecordDataset(filenames=[fsns_test_file])\n",
        "dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwpR5b5479up",
        "colab_type": "text"
      },
      "source": [
        "You can work with `tf.train.Example` protos outside of a `tf.data.Dataset` to understand data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBnmDx8-7V8X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "raw_example = next(iter(dataset))\n",
        "parsed = tf.train.Example.FromString(raw_example.numpy())\n",
        "\n",
        "feature = parsed.features.feature\n",
        "raw_image = feature['image/encoded'].bytes_list.value[0]\n",
        "img = tf.image.decode_png(raw_image)\n",
        "plt.imshow(img)\n",
        "plt.axis('off')\n",
        "_ = plt.title(feature['image/text'].bytes_list.value[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlaOk5RU8XQJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "raw_example = next(iter(dataset))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zHG_agH8jlT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tf_parse(eg):\n",
        "  example = tf.io.parse_example(\n",
        "      eg[tf.newaxis], {\n",
        "          'image/encoded': tf.io.FixedLenFeature(shape=(), dtype=tf.string),\n",
        "          'image/text': tf.io.FixedLenFeature(shape=(), dtype=tf.string)\n",
        "      })\n",
        "  return example['image/encoded'][0], example['image/text'][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yv17roSs8212",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img, txt = tf_parse(raw_example)\n",
        "print(txt.numpy())\n",
        "print(repr(image.numpy()[:20]), '...')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnAPQOke89UW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoded = dataset.map(tf_parse)\n",
        "decoded"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zO8QikR9FQa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image_batch, text_batch = next(iter(decoded.batch(10)))\n",
        "image_batch.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1id-zORT9wEM",
        "colab_type": "text"
      },
      "source": [
        "## Time series windowing\n",
        "Time series data is often organized with the time axis intact.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PaJUOTVA9Md6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use a simple `Dataset.range` to demonstrate:\n",
        "range_ds = tf.data.Dataset.range(100000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYSDhrKv-ctB",
        "colab_type": "text"
      },
      "source": [
        "Typically, models based on this sort of data will want a contiguous time slice.\n",
        "The simplest approact would be to batch the data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3DtO0Xj-aES",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Using batch\n",
        "batches = range_ds.batch(10, drop_remainder=True)\n",
        "\n",
        "for batch in batches.take(5):\n",
        "  print(batch.numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlYKUOXf_eJp",
        "colab_type": "text"
      },
      "source": [
        "## Skip Time series windowing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUYYqyJw-pt_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}