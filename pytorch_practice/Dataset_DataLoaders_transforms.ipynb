{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dataset_DataLoaders_transforms.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVxiQTsb0GHK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "from skimage import io, transform\n",
        "import skimage\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "plt.ion()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bezgYBEo0xBX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPVx8Esw1wFg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir data\n",
        "from shutil import unpack_archive\n",
        "unpack_archive('faces.zip', 'data/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhcKdbND1YtK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "landmarks_frame = pd.read_csv('data/faces/face_landmarks.csv')\n",
        "\n",
        "n = 50\n",
        "img_name = landmarks_frame.iloc[n, 0]\n",
        "landmarks = landmarks_frame.iloc[n, 1:].as_matrix()\n",
        "landmarks = landmarks.astype('float').reshape(-1, 2)\n",
        "\n",
        "print('Image name: {}'.format(img_name))\n",
        "print('Landmarks shape: {}'.format(landmarks.shape))\n",
        "print('First 4 Landmarks: {}'.format(landmarks[:4]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xK3-UbC12ps0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_landmarks(image, landmarks):\n",
        "  \"\"\"Show image with landmarks\"\"\"\n",
        "  plt.imshow(image)\n",
        "  plt.scatter(landmarks[:, 0], landmarks[:, 1], s=10, marker='.', c='r')\n",
        "  plt.pause(0.001) # Pause a bit so that plot are updated\n",
        "\n",
        "plt.figure()\n",
        "show_landmarks(io.imread(os.path.join('data/faces/', img_name)),\n",
        "               landmarks)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_awhesF3p-8",
        "colab_type": "text"
      },
      "source": [
        "### Dataset class\n",
        "`torch.utils.data.Dataset` is an abstract class representing a dataset. Your custom dataset should inherit `Dataset` and override the following methods:\n",
        "- `__len__` so that `len(dataset)` returns the size of the dataset.\n",
        "- `__getitem__` to support the indexing such that `dataset[i]` can be used to get i-th sample."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GGljYvK3REO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FaceLandmarksDataset(torch.utils.data.Dataset):\n",
        "  \"\"\"Face Landmarks dataset.\"\"\"\n",
        "\n",
        "  def __init__(self, csv_file, root_dir, transform=None):\n",
        "    \"\"\"\n",
        "      Args:\n",
        "        csv_file (string): Path to the csv file with annotations.\n",
        "        root_dir (string): Directory with all the images.\n",
        "        transform (callable, optional): Optional transform to be applied\n",
        "          one a sample.\n",
        "    \"\"\"\n",
        "    self.landmarks_frame = pd.read_csv(csv_file)\n",
        "    self.root_dir = root_dir\n",
        "    self.transform = transform\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.landmarks_frame)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    if torch.is_tensor(idx):\n",
        "      idx = idx.tolist()\n",
        "    \n",
        "    img_name = os.path.join(self.root_dir,\n",
        "                            self.landmarks_frame.iloc[idx, 0])\n",
        "    image = io.imread(img_name)\n",
        "    landmarks = self.landmarks_frame.iloc[idx, 1:]\n",
        "    landmarks = np.array([landmarks])\n",
        "    landmarks = landmarks.astype('float').reshape(-1, 2)\n",
        "    sample = {'image': image, 'landmarks': landmarks}\n",
        "\n",
        "    if self.transform is not None:\n",
        "      sample = self.transform(sample)\n",
        "\n",
        "    return sample"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4wUmZca5eT5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "face_dataset = FaceLandmarksDataset(csv_file='data/faces/face_landmarks.csv',\n",
        "                                    root_dir='data/faces/')\n",
        "\n",
        "fig = plt.figure()\n",
        "\n",
        "for i in range(len(face_dataset)):\n",
        "  sample = face_dataset[i]\n",
        "\n",
        "  print(i, sample['image'].shape, sample['landmarks'].shape)\n",
        "\n",
        "  ax = plt.subplot(1, 4, i + 1)\n",
        "  plt.tight_layout()\n",
        "  ax.set_title('Sample #{}'.format(i))\n",
        "  # ax.axis('off')\n",
        "  show_landmarks(**sample)\n",
        "\n",
        "  if i == 3:\n",
        "    plt.show()\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeQXrX_i7qnK",
        "colab_type": "text"
      },
      "source": [
        "### Transforms\n",
        "One issue we can see from the above is that the samples are not ot the size size. Most neural networks expect the images of a fixed size. Therefore, we will need to write some preprocessing code. \n",
        "- `Rescale`: to scale the image\n",
        "- `RandomCrop`: to crop from image randomly. This is data augmentation.\n",
        "- `ToTensor`: to convert the numpy images to torch images (we need to swap axes)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jc0PWkxy8JKl",
        "colab_type": "text"
      },
      "source": [
        "We will write them as callable classes instead of simple functions so that parameters of the transformer need not be passed everytime it's called. For this, we just need to implement `__call__` method and if required, `__init__` method.\n",
        "We can the use transform like:\n",
        "\n",
        "tsfm = Transform(params) \n",
        "\n",
        "\n",
        "transformed_sample = tsfm(sample)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W67_ZLQw6oyX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Rescale(object):\n",
        "  \"\"\"Rescale the image in a sample to given size.\n",
        "  \n",
        "  Args:\n",
        "    output_size (tuple or int): Desired output size. If tuple, output is\n",
        "    matched to output_size. If int, smaller of image edges is matched to\n",
        "    output_size keeping aspect ratio the same.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, output_size):\n",
        "    assert isinstance(output_size, (int, tuple))\n",
        "    self.output_size = output_size\n",
        "  \n",
        "  def __call__(self, sample):\n",
        "    image, landmarks = sample['image'], sample['landmarks']\n",
        "\n",
        "    h, w = image.shape[:2]\n",
        "    if isinstance(self.output_size, int):\n",
        "      if h > w:\n",
        "        new_h, new_w = self.output_size * h / w, self.output_size\n",
        "      else:\n",
        "        new_h, new_w = self.output_size, self.output_size * w / h\n",
        "    else:\n",
        "      new_h, new_w = self.output_size\n",
        "\n",
        "    new_h, new_w = int(new_h), int(new_w)\n",
        "    img = skimage.transform.resize(image, (new_h, new_w))\n",
        "    # h and w are swapped for landmarks because for images,\n",
        "    # x and y axes are axis 1 and 0 respectively\n",
        "    landmarks = landmarks * [new_w / w, new_h / h]\n",
        "\n",
        "    return {'image': img, 'landmarks': landmarks}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwnRz-Fy_mQQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RandomCrop(object):\n",
        "  \"\"\"Crop randomly the image in a sample.\n",
        "\n",
        "  Args:\n",
        "    output_size (tuple or int): Desired output size. If int, square crop is made.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, output_size):\n",
        "    assert isinstance(output_size, (int, tuple))\n",
        "    if isinstance(output_size, int):\n",
        "      self.output_size = (output_size, output_size)\n",
        "    else:\n",
        "      assert len(output_Size) == 2\n",
        "      self.output_size = output_size\n",
        "\n",
        "  def __call__(self, sample):\n",
        "    image, landmarks = sample['image'], sample['landmarks']\n",
        "\n",
        "    h, w = image.shape[:2]\n",
        "    new_h, new_w = self.output_size\n",
        "\n",
        "    top = np.random.randint(0, h - new_h)\n",
        "    left = np.random.randint(0, w - new_w)\n",
        "\n",
        "    image = image[top: top + new_h,\n",
        "                  left: left + new_w]\n",
        "\n",
        "    landmarks = landmarks - [left, top]\n",
        "\n",
        "    return {'image': image, 'landmarks': landmarks}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6B4v5kBDnSF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ToTensor(object):\n",
        "  \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
        "\n",
        "  def __call__(self, sample):\n",
        "    image, landmarks = sample['image'], sample['landmarks']\n",
        "\n",
        "    # Swap color axis because\n",
        "    # numpy image: H x W x C\n",
        "    # torch image: C x H x W\n",
        "    image = image.transpose((2, 0, 1))\n",
        "    return {'image': torch.from_numpy(image),\n",
        "            'landmarks': torch.from_numpy(landmarks)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5u5yFDnTENsf",
        "colab_type": "text"
      },
      "source": [
        "### Compose transforms\n",
        "\n",
        "We want to rescale the shorter side of the image to 256 and then randomly crop a square of size 224 from it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skaVWT4CEAXh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scale = Rescale(256)\n",
        "crop = RandomCrop(128)\n",
        "\n",
        "composed = torchvision.transforms.Compose([Rescale(256),\n",
        "                                           RandomCrop(224)])\n",
        "\n",
        "fig = plt.figure()\n",
        "sample = face_dataset[65]\n",
        "for i, tsfrm in enumerate([scale, crop, composed]):\n",
        "  transformed_sample = tsfrm(sample)\n",
        "\n",
        "  ax = plt.subplot(1, 3, i + 1)\n",
        "  plt.tight_layout()\n",
        "  ax.set_title(type(tsfrm).__name__)\n",
        "  show_landmarks(**transformed_sample)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YU7a7FgFWPq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transformed_dataset = FaceLandmarksDataset(csv_file='data/faces/face_landmarks.csv',\n",
        "                                           root_dir='data/faces/',\n",
        "                                           transform=torchvision.transforms.Compose([\n",
        "    Rescale(256),\n",
        "    RandomCrop(224),\n",
        "    ToTensor()\n",
        "]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFtZoOmWaeuc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(transformed_dataset)):\n",
        "  sample = transformed_dataset[i]\n",
        "\n",
        "  print(i, sample['image'].size(), sample['landmarks'].size())\n",
        "\n",
        "  if i == 3:\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCbfGCNNazgi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# torch.utils.data.DataLoader` is an iterator which\n",
        "# batching, shuffling, load the data in parallel\n",
        "dataloader = torch.utils.data.DataLoader(transformed_dataset, batch_size=4,\n",
        "                                         shuffle=True, num_workers=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jO-qPUk1cAWT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Helper function to show a batch\n",
        "def show_landmarks_batch(sample_batched):\n",
        "    \"\"\"Show image with landmarks for a batch of samples.\"\"\"\n",
        "    images_batch, landmarks_batch = \\\n",
        "            sample_batched['image'], sample_batched['landmarks']\n",
        "    batch_size = len(images_batch)\n",
        "    im_size = images_batch.size(2)\n",
        "    grid_border_size = 2\n",
        "\n",
        "    grid = utils.make_grid(images_batch)\n",
        "    plt.imshow(grid.numpy().transpose((1, 2, 0)))\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        plt.scatter(landmarks_batch[i, :, 0].numpy() + i * im_size + (i + 1) * grid_border_size,\n",
        "                    landmarks_batch[i, :, 1].numpy() + grid_border_size,\n",
        "                    s=10, marker='.', c='r')\n",
        "\n",
        "        plt.title('Batch from dataloader')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQdS2rZieCI6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i_batch, sample_batched in enumerate(dataloader):\n",
        "  print(i_batch, sample_batched['image'].size(), sample_batched['landmarks'].size())\n",
        "\n",
        "  if i_batch == 3:\n",
        "    plt.figure()\n",
        "    show_landmarks_batch(sample_batched)\n",
        "    plt.axis('off')\n",
        "    plt.ioff()\n",
        "    plt.show()\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jz87Ohg9dV20",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torchvision import transforms, datasets\n",
        "\n",
        "data_transform = transforms.Compose([transforms.RandomSizedCrop(224),\n",
        "                                     transforms.RandomHorizontalFlip(),\n",
        "                                     transforms.ToTensor(),\n",
        "                                     transforms.Normalize(mean=[0.485, 0.456, 0.405],\n",
        "                                                          std=[0.229, 0.224, 0.225])])\n",
        "\n",
        "dataset = datasets.ImageFolder(root='train', transform=data_transform)\n",
        "dataset_loader = torch.utils.data.DataLoader(dataset,\n",
        "                                             batch_size=4,\n",
        "                                             shuffle=True,\n",
        "                                             num_workers=4)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}